{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503585de-7619-47c7-a627-872789c23f1e",
   "metadata": {},
   "source": [
    "# Product Insight Validation Using LLMs üîç\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebooks aims to evaluate different prompting strategies for validating product insights using a Large Language Model (LLM). The goal is to determine the most effective prompting approach for distinguishing between valid and invalid insights based on predefined criteria.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- **Compare Prompting Strategies:** Test multiple prompts and strategies to determine which yields the best classification results.\n",
    "- **Evaluate Performance:** Measure the effectiveness of each strategy using precision, recall, and F1 score.\n",
    "- **Cross-Validation Approach:** Utilize a labeled dataset containing:\n",
    "  - **True Positives (TP):** Correctly identified valid insights.\n",
    "  - **True Negatives (TN):** Correctly identified invalid insights.\n",
    "  - **False Positives (FP):** Incorrectly marked invalid insights as valid.\n",
    "  - **False Negatives (FN):** Incorrectly marked valid insights as invalid.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "1. **Load Product Insights**  \n",
    "   - Import CSV files containing product insights for validation.\n",
    "\n",
    "2. **Apply LLM-Based Validation**  \n",
    "   - Use different prompts and prompting strategies to classify insights.\n",
    "\n",
    "3. **Evaluate Performance**  \n",
    "   - Compute precision, recall, and F1 score to assess classification accuracy.\n",
    "   - Compare the effectiveness of different strategies based on their performance metrics.\n",
    "\n",
    "4. **Optimize for Accuracy**  \n",
    "   - Identify the best-performing prompt and strategy for product insight validation.\n",
    "\n",
    "## Tech Stack\n",
    "\n",
    "- **LLM Provider:** Azure OpenAI  \n",
    "- **Model:** ChatGPT 4.0  \n",
    "- **Data Processing:** Python (pandas, numpy)  \n",
    "- **Evaluation Metrics:** precision, recall, F1 score  \n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "- A clear understanding of which prompting strategy yields the best results.\n",
    "- A methodology/workflow that can be iteratively improved and scaled for future product insight validation tasks.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "_This notebook has memes every once in a while. Jupyter notebooks are very nice but also can be a bit dry. The memes are not particularly good, don't judge me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6a5a9dc-0cbe-4f4c-bb91-51722615fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import the packages we will need for this project\n",
    "\n",
    "import requests # for connecting with Azure Open AI\n",
    "import json # for parsing responses\n",
    "import csv # for data processing\n",
    "import pandas as pd # for data analysis \n",
    "\n",
    "# let's also import the config we will need to interact with the Azure Open AI API\n",
    "\n",
    "from config import config_endpoint, config_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac873b2-0cbe-4539-ab03-41b717a6eeab",
   "metadata": {},
   "source": [
    "# 1 - Load Product Insights\n",
    "\n",
    "Let's take a glimpse at the data we have. All this data has been validated with an LLM with a custom prompt and then reviewed by human validators. This explains why we have true and false positives and negatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea37f835-c3be-426d-832a-f2891a6e5769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feedback</th>\n",
       "      <th>Product Feedback and Limitations validation_status</th>\n",
       "      <th>Product Feedback and Limitations comment</th>\n",
       "      <th>Product Feedback and Limitations_human_review</th>\n",
       "      <th>Product Feedback and Limitations_human_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feedback and limitations The customer expresse...</td>\n",
       "      <td>1</td>\n",
       "      <td>The feedback is specific as it refers to the r...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Impact on the customer's workflow stated. Acti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feedback and limitations The limitations of th...</td>\n",
       "      <td>1</td>\n",
       "      <td>The feedback is specific about compatibility i...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Valid feedback</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feedback and limitations Customer face difficu...</td>\n",
       "      <td>1</td>\n",
       "      <td>The feedback is valid as it specifies compatib...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feedback and limitations Cx was frustrated sin...</td>\n",
       "      <td>1</td>\n",
       "      <td>The feedback is valid as it specifies a limita...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feedback and limitations Product Limitation \\n...</td>\n",
       "      <td>1</td>\n",
       "      <td>The feedback is specific as it refers to the d...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Product feedback is specific and clear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Feedback  \\\n",
       "0  Feedback and limitations The customer expresse...   \n",
       "1  Feedback and limitations The limitations of th...   \n",
       "2  Feedback and limitations Customer face difficu...   \n",
       "3  Feedback and limitations Cx was frustrated sin...   \n",
       "4  Feedback and limitations Product Limitation \\n...   \n",
       "\n",
       "   Product Feedback and Limitations validation_status  \\\n",
       "0                                                  1    \n",
       "1                                                  1    \n",
       "2                                                  1    \n",
       "3                                                  1    \n",
       "4                                                  1    \n",
       "\n",
       "            Product Feedback and Limitations comment  \\\n",
       "0  The feedback is specific as it refers to the r...   \n",
       "1  The feedback is specific about compatibility i...   \n",
       "2  The feedback is valid as it specifies compatib...   \n",
       "3  The feedback is valid as it specifies a limita...   \n",
       "4  The feedback is specific as it refers to the d...   \n",
       "\n",
       "  Product Feedback and Limitations_human_review  \\\n",
       "0                                         Agree   \n",
       "1                                         Agree   \n",
       "2                                         Agree   \n",
       "3                                         Agree   \n",
       "4                                         Agree   \n",
       "\n",
       "      Product Feedback and Limitations_human_comment  \n",
       "0  Impact on the customer's workflow stated. Acti...  \n",
       "1                                     Valid feedback  \n",
       "2                                              Valid  \n",
       "3                                                NaN  \n",
       "4             Product feedback is specific and clear  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We load all the data \n",
    "\n",
    "true_positives = pd.read_csv('true_positive_sample.csv')\n",
    "true_negatives = pd.read_csv('true_negative_sample.csv')\n",
    "false_positives = pd.read_csv('false_positive_sample.csv')\n",
    "false_negatives = pd.read_csv('false_negative_sample.csv')\n",
    "\n",
    "# Now let's print one of the datasets to see its shape\n",
    "\n",
    "true_positives[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a622af27-5a45-4d55-92be-76c5b75a9a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column Name</th>\n",
       "      <th>Explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feedback</td>\n",
       "      <td>Raw feedback notes captured by the agent and stored on Gigplus Trackers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product Feedback and Limitations validation_status</td>\n",
       "      <td>Validation done by the LLM - 0 is invalid, 1 is valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Product Feedback and Limitations comment</td>\n",
       "      <td>Explanation provided by the LLM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Product Feedback and Limitations_human_review</td>\n",
       "      <td>Human review, agreeing or disagreeing with the model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product Feedback and Limitations_human_comment</td>\n",
       "      <td>Comment left by the human validator</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Column Name  \\\n",
       "0                                            Feedback   \n",
       "1  Product Feedback and Limitations validation_status   \n",
       "2            Product Feedback and Limitations comment   \n",
       "3       Product Feedback and Limitations_human_review   \n",
       "4      Product Feedback and Limitations_human_comment   \n",
       "\n",
       "                                                               Explanation  \n",
       "0  Raw feedback notes captured by the agent and stored on Gigplus Trackers  \n",
       "1                    Validation done by the LLM - 0 is invalid, 1 is valid  \n",
       "2                                          Explanation provided by the LLM  \n",
       "3                     Human review, agreeing or disagreeing with the model  \n",
       "4                                      Comment left by the human validator  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column explanation\n",
    "data = [\n",
    "    [\"Feedback\", \"Raw feedback notes captured by the agent and stored on Gigplus Trackers\"],\n",
    "    [\"Product Feedback and Limitations validation_status\", \"Validation done by the LLM - 0 is invalid, 1 is valid\"],\n",
    "    [\"Product Feedback and Limitations comment\", \"Explanation provided by the LLM\"],\n",
    "    [\"Product Feedback and Limitations_human_review\", \"Human review, agreeing or disagreeing with the model\"],\n",
    "    [\"Product Feedback and Limitations_human_comment\", \"Comment left by the human validator\"]\n",
    "]\n",
    "column_data = pd.DataFrame(data, columns=[\"Column Name\", \"Explanation\"])\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None) \n",
    "column_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ffb247-9a9f-4505-8b68-10aae8ac23a8",
   "metadata": {},
   "source": [
    "## 1.1 Baselining Performance\n",
    "\n",
    "These 4 datasets have already been evaluated by an LLM as well as been reviewed by a human.\n",
    "\n",
    "This means we can calculate Sensitivity, Recall and F1 for this dataset, which will give us target performance metrics to iterate on. Let's refresh on how these are calculated\n",
    "\n",
    "# Performance Metrics\n",
    "\n",
    "## Sensitivity (Recall)\n",
    "Sensitivity, also known as **recall**, measures the ability to correctly identify positive cases:\n",
    "\n",
    "$$\n",
    "\\text{Sensitivity} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}\n",
    "$$\n",
    "\n",
    "## Precision\n",
    "Precision measures how many of the predicted positive cases were actually correct:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}\n",
    "$$\n",
    "\n",
    "## F1 Score\n",
    "F1 Score is the harmonic mean of precision and recall, balancing both metrics:\n",
    "\n",
    "$$\n",
    "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Sensitivity}}{\\text{Precision} + \\text{Sensitivity}}\n",
    "$$\n",
    "\n",
    "\n",
    "With that, let's calculate sensitivity, precision and F1 score for our current dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "237f8dc1-de9e-4064-969c-74312b37e9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F1_Score</td>\n",
       "      <td>0.645161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Metric     Value\n",
       "0  Sensitivity  0.500000\n",
       "1    Precision  0.909091\n",
       "2     F1_Score  0.645161"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp = true_positives.shape[0]  \n",
    "tn = true_negatives.shape[0]\n",
    "fp = false_positives.shape[0]\n",
    "fn = false_negatives.shape[0]\n",
    "\n",
    "sensitivity = tp / ( tp + fn )\n",
    "precision = tp / ( tp + fp )\n",
    "f_1 = 2 * ( precision * sensitivity ) / ( precision + sensitivity )\n",
    "\n",
    "baseline_eval_metrics = pd.DataFrame([\n",
    "    [\"Sensitivity\", sensitivity],\n",
    "    [\"Precision\", precision],\n",
    "    [\"F1_Score\", f_1]\n",
    "    ],\n",
    "    columns=[\"Metric\", \"Value\"]\n",
    ")\n",
    "\n",
    "baseline_eval_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79441836-5541-4417-954a-f158c16499da",
   "metadata": {},
   "source": [
    "The metrics are very low and in principle \"easy to beat\", but this is only because the sample size is very small for true positives and true negatives. \n",
    "\n",
    "In reality, the previous model performed better than this - nevertheless, this gives us a compass for our exercise.\n",
    "\n",
    "**New prompts/prompt strategies should be able to have a better ability to catch false positives and false negatives while maintaining accuracy with true positives and negatives**\n",
    "\n",
    "We'll store the result of all our tests into a dataframe table. This will allow us to contrast and compare approaches and make a final selection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe892272-381c-41f5-af17-08e06550115f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xv/5lp8ff8s7j55zh1lrpr9ddkw0000gn/T/ipykernel_36922/125464560.py:3: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_results = pd.concat([test_results, pd.DataFrame({\"test_name\": [\"original\"] ,\"sensitivity\": [sensitivity] , \"precision\": [precision] , \"f1_score\": [f_1]  })], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_name</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>original</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.645161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  test_name  sensitivity  precision  f1_score\n",
       "0  original          0.5   0.909091  0.645161"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results = pd.DataFrame([], columns=[\"test_name\", \"sensitivity\", \"precision\", \"f1_score\"])\n",
    "\n",
    "test_results = pd.concat([test_results, pd.DataFrame({\"test_name\": [\"original\"] ,\"sensitivity\": [sensitivity] , \"precision\": [precision] , \"f1_score\": [f_1]  })], ignore_index=True)\n",
    "\n",
    "test_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb9dfaf-80e7-4e4d-8eeb-ab90d40ec221",
   "metadata": {},
   "source": [
    "## 2. Setting Up Logic for LLM Validation and Analysis\n",
    "\n",
    "### 2.1 Validation\n",
    "\n",
    "Let's start this section by defining a function that calls Azure Open AI with a system prompt, and an input provided by the user. \n",
    "\n",
    "The system prompt will contain the criteria to validate an insight, and the user input will be the entry registered by our agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b8fab4b-9724-4a7d-80c7-6258db4c9e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": config_key\n",
    "}\n",
    "\n",
    "def send_prompt(system_prompt, user_prompt, max_tokens=200):\n",
    "    \"\"\"Send a prompt to Azure OpenAI and return the response.\"\"\"\n",
    "    url = config_endpoint\n",
    "    data = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=HEADERS, json=data)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb52e7-0ebb-4809-ace7-ca6a43ab23cb",
   "metadata": {},
   "source": [
    "Let's test it out with a very naive example to make sure it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b865821-a50d-443f-88b3-48ecdccea9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reason\": \"The feedback directly addresses a usability issue related to the app's menu being convoluted, crowded with icons, and hard to read. These factors impact the ability to use the app effectively.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "res = send_prompt(\n",
    "    \"You are system dedicated to validate product feedback. You will only declare as valid feedback that has to do usability issues, anything else will be invalid. Always return json with two fields: { valid: can only be true or false, reason: your reasoning as to why the insight is valid or invalid }\",\n",
    "    \"I could not use the app at all, the menu was very convoluted and crowded with icons. Very hard to read\"\n",
    ")\n",
    "\n",
    "print(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a825f309-c7c8-4ebf-8cf3-41f5c051eaf1",
   "metadata": {},
   "source": [
    "The model is giving us back a string formatted in Markdown. Let's create a function to clean it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c839bbdc-1bac-4670-bce4-4eafbdd429ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_llm_response(res):\n",
    "    return res.replace(\"json\", \"\").replace(r'\\n', '').replace(r\"\\'\", \"'\").replace(\"`\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "230f7637-2091-4ac8-bf78-a97087ec0f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"valid\": true,\n",
      "  \"reason\": \"The feedback directly addresses a usability issue related to the app's menu being convoluted, crowded with icons, and hard to read. These factors impact the ability to use the app effectively.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(clean_llm_response(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e3e61-d54e-4363-a040-2d5ddb7f12b1",
   "metadata": {},
   "source": [
    "Great! We now have the basic building block for testing different validation prompts.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049ff6ea-b393-48bd-a11b-f460424eadad",
   "metadata": {},
   "source": [
    "###  2.2 Analysis of prompt performance\n",
    "\n",
    "Now we need a function that allows us to do the following:\n",
    "\n",
    "- 1. Iterate through our TP, TN, FP, FN datasets.\n",
    "- 2. For each of the rows in each of the datasets\n",
    "    - 1. Ask the LLM to validate the product feedback entry\n",
    "    - 2. Evaluate if the LLM did a good job or not\n",
    "    - 3. Store this information\n",
    "- 4. Calculate Sensitivity, Recall and F1 for this prompt\n",
    "- 5. Add the results to our log in the `test_results` variable we created before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d09ae07-7b05-47d6-ba7d-bd220aab3891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import ipdb;\n",
    "\n",
    "def analyse_test_prompt(test_name, prompt, results_store):\n",
    "    '''\n",
    "    Evaluates the performance of a prompt \n",
    "\n",
    "    Args:\n",
    "      - test_name: name of the test, can be used as an identifier\n",
    "      - prompt: system prompt passed to the LLM to validate product feedback\n",
    "      - results_store: dataframe where we can store the results\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    \n",
    "    dataframes = [\n",
    "        [true_positives, True], # first value contains the data, the second what we would like the model to return for every row\n",
    "        [true_negatives, False], # for instance, the llm should evaluate all true positives as valid to have 100% accuracy \n",
    "        [false_positives, False],\n",
    "        [false_negatives, True]\n",
    "    ]\n",
    "\n",
    "    # counters to evaluate metrics\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    row_counter = 0\n",
    "    for dataframe in dataframes:\n",
    "        data, expectation = dataframe\n",
    "        \n",
    "        for index, row in data.iterrows():\n",
    "            # avoid token limit if needed every 10 rows \n",
    "            print(row_counter)\n",
    "            if row_counter >= 10:\n",
    "                print(f\"Rate limit is close, continuing in {60} seconds...\")\n",
    "                time.sleep(61)\n",
    "                row_counter = 0\n",
    "                \n",
    "                \n",
    "            llm_res = send_prompt(prompt, row['Feedback'])\n",
    "            llm_res = clean_llm_response(llm_res)\n",
    "            row_counter += 1\n",
    "            print(llm_res)\n",
    "            \n",
    "            llm_res = json.loads(llm_res)\n",
    "            if expectation == True and llm_res['valid'] == True:\n",
    "                tp += 1\n",
    "            elif expectation == True and llm_res['valid'] == False:\n",
    "                fn += 1\n",
    "            elif expectation == False and llm_res['valid'] == True:\n",
    "                fp += 1\n",
    "            elif expectation == False and llm_res['valid'] == False:\n",
    "                tn += 1\n",
    "\n",
    "    sensitivity = tp / ( tp + fn )\n",
    "    precision = tp / ( tp + fp )\n",
    "    f_1 = 2 * ( precision * sensitivity ) / ( precision + sensitivity )\n",
    "    new_results_row = pd.DataFrame({\"test_name\": [test_name] ,\"sensitivity\": [sensitivity] , \"precision\": [precision] , \"f1_score\": [f_1]  })\n",
    "\n",
    "    test_values = results_store[\"test_name\"].values\n",
    "    \n",
    "    if test_name in test_values:\n",
    "        index_to_replace = results_store[results_store[\"test_name\"] == test_name].index[0]\n",
    "        results_store.loc[index_to_replace] = new_results_row.iloc[0] \n",
    "    else:\n",
    "        results_store = pd.concat([results_store, new_results_row], ignore_index=True)\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time) \n",
    "    return results_store\n",
    "\n",
    "test_prompt = \"You are system dedicated to validate product feedback. You will only declare as valid feedback that has to do usability issues, anything else will be invalid. Always return json with two fields: { valid: can only be true or false, reason: your reasoning as to why the insight is valid or invalid }\"\n",
    "\n",
    "# test_results = analyse_test_prompt('testy test', test_prompt, test_results)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc35f49-c179-43b4-a4e9-53e448dade76",
   "metadata": {},
   "source": [
    "Ok, our building blocks of logic are now ready. Let's start with some prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1390c5-acb7-433a-a7e7-fd23476370bb",
   "metadata": {},
   "source": [
    "## 3. Testing Prompting Approaches\n",
    "\n",
    "In this section we will test the performance of several prompting approaches to see which one seems performs better. Let's go!\n",
    "\n",
    "\n",
    "### 3.1 Zero Shot Prompt\n",
    "\n",
    "Zero-shot prompting is a technique used with large language models (LLMs) where the model is asked to perform a task without being given any specific examples of how to do it. We're relying entirely on the model's pre-existing knowledge and understanding to generate a response. ¬†\n",
    "\n",
    "In the prompt below, we describe 2 sets of criteria, one for Product Feedback and Limitations, and another one for Deployment Blockers. We instruct the model to validate the entries when 1 of the criteria sets are met.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "063cf567-e259-4371-a948-ce87cac5485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_prompt = '''\n",
    "You are an AI assistant that validates entries based on specific criteria. \n",
    "\n",
    "Your job is to mark any entries given to you as valid or invalid. An input will be valid whenever it conforms to any of the following sets of criteria.\n",
    "\n",
    "## Set 1: Product Feedback and Limitations Criteria\n",
    "\n",
    "Meeting criteria A) and B) is a must have for the entry to be considered valid\n",
    "\n",
    "    - A) Actionability: the entry mentions product feedback, limitations that are specific, actionable and valuable for a product team.\n",
    "\n",
    "    - B) Specificity: The entry should clearly refer to a specific product feature\n",
    "    \n",
    "Meeting at least one of the following criteria C), D) and E) is enough to check the entry as valid\n",
    "\n",
    "    - C) Support of Objectives: The entry should explain how the feedback aligns customer business objectives or business case, regardless of whether the feedback is positive or negative. \n",
    "\n",
    "    - D) Impact on Customer Experience: The entry must explain how it impacts customer workflows, satisfaction, or any stage of the customer's experience\n",
    "\n",
    "    - E) Usability: The entry explains how a feature of the product is difficult to use\n",
    "    \n",
    "    - F) Positive Feedback: The entry provides positive feedback about a feature or aspect of the product\n",
    "\n",
    "##Set 1 end##\n",
    "      \n",
    "##Set 2: Valid Deployment Blockers ## \n",
    "\n",
    "Meeting any of the following criteria is enough to check the entry as valid\n",
    "\n",
    "    - Technical Barriers: The entry contains obstacles that prevents or limits the successful implementation, adoption, or performance of a technology, system or product.  \n",
    "\n",
    "    - Organizational Readiness: The entry refers to a shortage of trained personnel or expertise to adopt, implement or maintain a the product.\n",
    "\n",
    "    - Compatibility:  The entry explains how the product cannot be adopted or used due to lack of compatibility, outdated systems, or proprietary formats \n",
    "\n",
    "    - Support and Documentation: The entry explains how poor documentation prevents the deployment, adoption or use of the product \n",
    "\n",
    "    - Security and Compliance: The entry explains risks related to data protection, cybersecurity threats, or compliance with privacy laws that prevent deployment, adoption or use of the product.\n",
    "\n",
    "## Set 2 end ## \n",
    "\n",
    "Meeting the criteria of one of the sets is enough to consider an entry as valid. \n",
    "\n",
    "## Response ##\n",
    "\n",
    "You will respond in JSON format with the following fields:\n",
    "* valid - make it true if the entry is considered valid, false if invalid\n",
    "* reasoning - add your reasoning based on the criteria set above\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2352fe6-b726-430a-8e4d-fb5ce59bc8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.3721399307251\n"
     ]
    }
   ],
   "source": [
    "test_results = analyse_test_prompt('zero shot', zero_shot_prompt, test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "424870dd-e49f-4291-8468-296bab35f024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    test_name  sensitivity  precision  f1_score\n",
      "0    original          0.5   0.909091  0.645161\n",
      "1  testy test          0.7   1.000000  0.823529\n",
      "2   zero shot          1.0   0.714286  0.833333\n"
     ]
    }
   ],
   "source": [
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797916cf-f235-41a0-b381-9ae277a75bca",
   "metadata": {},
   "source": [
    "### 3.1 Few-Shot Prompt\n",
    "\n",
    "Few-shot prompting is a technique in prompt engineering that aims to augment LLMs by providing a small number of examples within the prompt itself. This allows the model to learn and adapt to a specific task without requiring extensive fine-tuning.\n",
    "\n",
    "In the prompt below, we will provide a few positive and negative examples for each of the categories, and see its impact on performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ec3aae5-8c70-4d8e-a223-7c12f6daf1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = '''\n",
    "\n",
    "You are an AI assistant that validates entries based on specific criteria. \n",
    "\n",
    "Your job is to mark any entries given to you as valid or invalid. An input will be valid whenever it conforms to any of the following sets of criteria.\n",
    "\n",
    "## Set 1: Product Feedback and Limitations Criteria\n",
    "\n",
    "Meeting criteria A) and B) is a must have for the entry to be considered valid\n",
    "\n",
    "    - A) Actionability: the entry mentions product feedback, limitations that are specific, actionable and valuable for a product team.\n",
    "\n",
    "    - B) Specificity: The entry should clearly refer to a specific product feature or a product limitation\n",
    "    \n",
    "Meeting at least one of the following criteria C), D) and E) is enough to check the entry as valid\n",
    "\n",
    "    - C) Support of Objectives: The entry should explain how the feedback aligns customer business objectives or business case, regardless of whether the feedback is positive or negative. \n",
    "\n",
    "    - D) Impact on Customer Experience: The entry must explain how it impacts customer workflows, satisfaction, or any stage of the customer's experience\n",
    "    \n",
    "    - E) Positive Feedback: The entry provides positive feedback about a feature or aspect of the product\n",
    "\n",
    "##Set 1 end##\n",
    "      \n",
    "##Set 2: Valid Deployment Blockers## \n",
    "\n",
    "Meeting any of the following criteria is enough to check the entry as valid\n",
    "\n",
    "    - Technical Barriers: The entry contains concrete obstacles that prevents or limits the successful implementation, adoption, or performance of a technology, system or product.  \n",
    "\n",
    "    - Organizational Readiness: The entry refers to a shortage of trained personnel or expertise to adopt, implement or maintain a the product.\n",
    "\n",
    "    - Compatibility:  The entry explains clearly how the product cannot be adopted or used due to lack of compatibility, outdated systems, or proprietary formats \n",
    "\n",
    "    - Support and Documentation: The entry explains how poor documentation prevents the deployment, adoption or use of the product \n",
    "\n",
    "    - Security and Compliance: The entry explains risks related to data protection, cybersecurity threats, or compliance with privacy laws that prevent deployment, adoption or use of the product.\n",
    "\n",
    "## Set 2 end ## \n",
    "\n",
    "## Examples of valid Entries for Set 1 ## \n",
    "\n",
    "    - ‚ÄúThe Product seems very hard to use, doing basic actions like managing the calendar requires many clicks and it's confusing.‚Äù ‚Äì [ Valid, Meets Criteria A), B) and D) ]\n",
    "\n",
    "    - ‚ÄúThe Product is not able to perform scheduled updates, forcing the customer to do manual work and waste time‚Äù ‚Äì [ Valid, Meets Criteria A), B) and C) ]\n",
    "\n",
    "    - ‚ÄúThe customer mentioned how happy he was with the new data visualization suite and the FabricView feature. It has really helped his team make better decisions‚Äù ‚Äì [ Valid, Meets Criteria A), B) and E) ]\n",
    "\n",
    "    - ‚ÄúThe customer was frustrated because the product is unstable when running alongside another application‚Äù ‚Äì [ Valid - Meets criteria A) and B) and D) ]\n",
    "\n",
    "## Examples of valid Entries for Set 1 end ##\n",
    "\n",
    "## Examples of invalid Entries for Set 1 ## \n",
    "\n",
    "    - ‚ÄúThe Product seems slow sometimes.‚Äù ‚Äì [ Invalid - Does not meet criteria A) and B) ]\n",
    "\n",
    "    - ‚ÄúThe customer does not like the product, he prefers the older version. Also thinks the competition is better‚Äù ‚Äì [ Invalid - Does not meet criteria A) and B) ]\n",
    "\n",
    "    - ‚ÄúWe heard from another company that they had issues with the product.‚Äù ‚Äì [ Invalid, Does not meet criteria A) and B) ]  \n",
    "\n",
    "    - ‚ÄúWe really love the product, the new functionalities are really cool and helps us make more money which is what we want‚Äù ‚Äì [ Invalid, Does not meet criteria A) and B) even though meets C) and E) ] \n",
    "\n",
    "    - ‚ÄúWe need time to adjust to new workflows.‚Äù - [ Invalid, Does not meet criteria A) and B) ]\n",
    "\n",
    "    - \"Product is a bit expensive, should rethink the price point\" - [ Invalid, Does not meet criteria A) and B) ]\n",
    "\n",
    "## Examples of invalid Entries for Set 1 end ##\n",
    "\n",
    "## Examples of valid Entries for Set 2 ## \n",
    "\n",
    "    - ‚ÄúThe customer raised a concern about the lack of multi-tenancy support. They need a way to manage multiple teams and departments separately within the product.‚Äù ‚Äì [ Valid, Technical Barrier ]\n",
    "\n",
    "    - ‚ÄúThe customer said that while they see the value in our solution, they can't deploy because their team would need extensive training to use it effectively.‚Äù ‚Äì [ Valid, Organizational Readiness ]\n",
    "\n",
    "    - ‚Äúhe customer mentioned that they were excited to deploy the product, but they discovered it's not compatible with their existing infrastructure. Their systems run on Linux, while the software only supports Windows, making it impossible for them to implement‚Äù ‚Äì [ Valid, Compatibility ]\n",
    "\n",
    "    - ‚ÄúThe customer said that when they encountered an issue, they couldn‚Äôt find sufficient troubleshooting guides or FAQs to resolve it on their own, making them overly reliant on support.‚Äù ‚Äì [ Valid, Support and Documentation ]\n",
    "\n",
    "    - \"The product only provides US-based data hosting but the customer requires GDPR, so legally they cannot use it.\" - [Valid, Security and Compliance]\n",
    "\n",
    "## Examples of valid Entries for Set 2 end ##\n",
    "\n",
    "## Examples of invalid Entries for Set 2 ## \n",
    "\n",
    "    - ‚ÄúOur office is moving next month, so we can‚Äôt focus on deployment right now.‚Äù ‚Äì [ Invalid, Does not meet any of the criteria ]\n",
    "\n",
    "    - ‚ÄúThe system doesn't seem to work as expected in our environment.‚Äù ‚Äì [ Invalid, Does not meet any of the criteria ]\n",
    "\n",
    "    - ‚ÄúThe customer mentioned that they are facing some challenges with the new system.‚Äù ‚Äì [ Invalid, Does not meet any of the criteria ]\n",
    "\n",
    "    - ‚ÄúThe customer said that they are not sure how to proceed with the migration.‚Äù ‚Äì [ Invalid, Does not meet any of the criteria ]\n",
    "\n",
    "    - \"They have concerns about security that need to be cleared before they proceed with the deployment\" - [Invalid, Does not meet any of the criteria]\n",
    "\n",
    "## Examples of valid Entries for Set 2 end ##\n",
    "\n",
    "Meeting the criteria of one of the sets is enough to consider an entry as valid. You must emit a final single judgement \n",
    "\n",
    "## Response ##\n",
    "\n",
    "You will always respond in JSON format with only the following fields and no more:\n",
    "\n",
    "* valid - make it true if the entry is considered valid, false if invalid\n",
    "* reasoning - add your reasoning based on the criteria set above\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f848942d-fe60-4fbb-ac79-38ce92d4a4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "ERROR! Session/line number was not unique in database. History logging moved to new session 2\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The entry is valid under Set 1 criteria. It satisfies A) Actionability, as the feedback is actionable and valuable to the product team, and B) Specificity, as it specifically refers to the removal of the add-on for email encryption. Furthermore, it meets D) Impact on Customer Experience, as it explains how the removal complicates the customer's workflow and forces them to take additional steps for secure communication. The entry does not fit the criteria for deployment blockers.\"\n",
      "}\n",
      "1\n",
      "{\n",
      "  \"valid\": false,\n",
      "  \"reasoning\": \"The entry touches upon compatibility issues and user experience differences between app versions and desktop versions. However, it lacks clear actionability and specificity related to a specific feature or limitation, which are required for Set 1 criteria A) and B) to be met. While it mentions compatibility issues, it does not explicitly explain how it prevents adoption or deployment as required for Set 2. It essentially doesn't fully meet the requirements of either Set 1 or Set 2.\"\n",
      "}\n",
      "2\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The entry is valid under Set 2: Valid Deployment Blockers. It addresses both 'Compatibility' by mentioning issues with integrating Microsoft Teams with other apps due to missing standard APIs, and 'Support and Documentation' by noting the limited documentation available. These are specific and concrete barriers that prevent smooth adoption and use of the product. Additionally, it highlights workflow inefficiencies, data synchronization issues, and increased support requests as impacts of these blockers.\"\n",
      "}\n",
      "3\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"This entry is considered valid under Set 2: Valid Deployment Blockers. It meets the 'Support and Documentation' criterion because the customer is frustrated due to insufficient information in Microsoft documentation. The lack of clarity about potential data loss from adding the domain to their M365 tenant and how emails will be redirected prevents them from moving forward with deployment.\"\n",
      "}\n",
      "4\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The entry meets the criteria for Set 1: Product Feedback and Limitations. Specifically, it fulfills A) Actionability by providing clear, actionable feedback about the GA role functionality where the customer wants other roles to be automatically disabled after assigning the GA role. It also meets B) Specificity as it refers explicitly to a specific product limitation regarding role assignment. Additionally, it meets D) Impact on Customer Experience by explaining how the current functionality affects workflows and usability for non-technical users. Therefore, the entry is valid.\"\n",
      "}\n",
      "5\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The entry is valid as it meets the 'Compatibility' criterion under Set 2. It states that the Microsoft personal account is not compatible with the third-party software Recruiterflowcomreset API and requires a Microsoft 365 Business account to function properly. This clearly highlights a compatibility issue, which is a valid deployment blocker.\"\n",
      "}\n",
      "6\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The entry is valid as it meets the criteria of both sets. For Set 1, it satisfies A) Actionability and B) Specificity by pointing out a specific limitation (the inability to approve a migration batch without Microsoft support). It also aligns with D) Impact on Customer Experience, as this limitation could lead to inefficiencies or delays in user workflows. For Set 2, it qualifies under 'Technical Barriers' because it highlights a specific technical limitation preventing smooth implementation or adoption of the product.\"\n",
      "}\n",
      "7\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The entry meets the criteria for Set 1: Product Feedback and Limitations. It specifies a limitation regarding SharePoint integration with Finder and provides actionable feedback about how it differs from Dropbox‚Äîfocusing on multiple locations versus a single location with maintained folder structure. It is specific in identifying a product limitation, thus meeting criteria A) Actionability and B) Specificity. Additionally, it explains how this impacts customer workflows and usability, meeting criteria D) Impact on Customer Experience.\"\n",
      "}\n",
      "8\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The entry is valid as it meets all required criteria A) Actionability and B) Specificity, clearly mentioning the limitation of the Teams web app regarding merging calls. It also meets criterion D) Impact on Customer Experience, as it describes how the limitation affects workflows by hindering collaboration to discuss appointments or patient status. Additionally, it makes a specific request for the feature's availability in the Teams web app, aligning it with actionable feedback.\"\n",
      "}\n",
      "9\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The entry is valid under Set 1: Product Feedback and Limitations. It meets criteria A) Actionability, as the feedback is specific and actionable for the product team, suggesting a feature to mitigate privacy concerns. It also meets criterion B) Specificity, as it clearly refers to a specific product feature - the Chat panel. Additionally, it aligns with criterion D) Impact on Customer Experience since it discusses how this limitation impacts privacy and user workflows by potentially exposing private conversations and causing unintentional read receipts. Therefore, it meets the requirements for Set 1 validation. For Set 2: Deployment Blockers, however, this doesn't qualify as it focuses on product functionality and is not explicitly related to deployment obstacles such as technical barriers, organizational readiness, compatibility, documentation, or compliance.\"\n",
      "}\n",
      "10\n",
      "Rate limit is close, continuing in 60 seconds...\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The entry is valid based on Set 1 (Criteria for Valid Product Feedback and Limitations). It meets the required A) Actionability and B) Specificity criteria because it mentions a specific limitation (lack of an option to monitor on-prem health for hybrid environments) and provides actionable feedback for improvement. Additionally, it aligns with D) Impact on Customer Experience, as this feature would likely improve workflows and user satisfaction in hybrid environments. Set 2 does not apply here as no deployment blockers are mentioned.\"\n",
      "}\n",
      "1\n",
      "{\n",
      "  \"valid\": false,\n",
      "  \"reasoning\": \"The entry primarily describes a conversation about transitioning from Gmail to Outlook and the benefits of doing so, but it does not specifically highlight actionable product feedback, product limitations, or blockers as required by the criteria of Set 1 or Set 2. While it touches on features of Outlook and Microsoft 365, it does not provide new feedback or detail specific barriers preventing deployment, implementation, or usage (e.g., technical barriers, compatibility issues, poor documentation). Therefore, it does not meet the criteria for either Set 1 or Set 2.\"\n",
      "}\n",
      "2\n",
      "{\n",
      "  \"valid\": false,\n",
      "  \"reasoning\": \"The entry does not contain any information or context regarding product feedback, limitations, or deployment blockers. Without specific details, it cannot meet the criteria defined in Set 1 or Set 2.\"\n",
      "}\n",
      "3\n",
      "{\n",
      "  \"valid\": false,\n",
      "  \"reasoning\": \"The entry does not provide enough information or specificity to meet the criteria in either Set 1 (Product Feedback and Limitations) or Set 2 (Valid Deployment Blockers). For Set 1, it does not explain a specific product feature or limitation in actionable terms nor does it meet criteria C, D, or E. For Set 2, it does not specify concrete technical barriers, compatibility issues, organizational readiness gaps, or other blockers outlined in the criteria.\"\n",
      "}\n",
      "4\n",
      "{\n",
      "  \"valid\": false,\n",
      "  \"reasoning\": \"The entry is too vague and does not meet the criteria for either Set 1 or Set 2. For Set 1, it does not clearly mention specific product feedback or limitations that are actionable and valuable for a product team (Criteria A), nor does it refer to a specific product feature or limitation (Criteria B). It also fails to meet Criteria C), D), or E). For Set 2, it does not provide concrete technical barriers, organizational readiness, compatibility issues, support/documentation gaps, or security/compliance concerns that would constitute a deployment blocker.\"\n",
      "}\n",
      "5\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The entry meets the criteria for Set 1 (Valid Product Feedback and Limitations). The feedback provides specific and actionable insight: the customer is asking for a feature to auto-login Google accounts in the Outlook desktop app or across devices. This aligns with criteria A) Actionability and B) Specificity. Additionally, it impacts customer workflows and satisfaction, meeting criterion D) Impact on Customer Experience. While deployment blockers are mentioned in the input, no specific blockers are described, so that part cannot be evaluated.\"\n",
      "}\n",
      "6\n",
      "{\n",
      "  \"valid\": false,\n",
      "  \"reasoning\": \"The feedback does not meet the criteria for Set 1 or Set 2. While it mentions the user being happy with Microsoft products, it lacks specificity about a specific product feature or limitation (criteria B). Additionally, there is no actionable or valuable feedback for a product team (criteria A). Furthermore, the mention of deployment blockers being 'none' does not meet any of the criteria for Set 2.\"\n",
      "}\n",
      "7\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The entry is valid under Set 1, Product Feedback and Limitations Criteria. It meets A) Actionability, as it provides actionable feedback about the difficulty in editing meeting links, B) Specificity as it refers to a specific product feature related to meeting link editing, and D) Impact on Customer Experience as it affects the customer's workflow when sharing links through Outlook.\"\n",
      "}\n",
      "8\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The entry is valid because it meets both Set 1 and Set 2 criteria. For Set 1: It mentions product feedback that's actionable, specific, and valuable (payment issues and subscription verification challenges), fulfilling criteria A) and B). It also satisfies D) because the mentioned issues impact customer workflows and experience. For Set 2: The entry refers to 'verification or purchasing subscriptions,' which can qualify as Technical Barriers that block deployment or smooth functioning of the product.\"\n",
      "}\n",
      "9\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"This entry is valid as feedback under Set 1 criteria because it describes specific limitations (migration process, setup, and configuration) which are actionable and valuable for a product team (meeting criteria A and B). Additionally, it explains the potential impact on customer experience and efficiency during implementation (meeting criteria D). It does not specifically meet the criteria for Set 2 deployment blockers but fulfills the criteria for Set 1, making it valid overall.\"\n",
      "}\n",
      "10\n",
      "Rate limit is close, continuing in 60 seconds...\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The entry is valid as it meets the criteria for both Set 1: Product Feedback and Limitations and Set 2: Valid Deployment Blockers. For Set 1, it meets criteria A) because it provides actionable and specific feedback about the inability of trial users to chat or communicate with external parties, which is a product limitation. It meets criteria B) as it specifies the limitation clearly. It also meets criterion C) as it describes how this impacts customer business objectives by mentioning the need to demonstrate the functionality to clients for purchase decisions. For Set 2, it is valid because it presents a 'Technical Barrier,' describing how the limitation in trial functionality hinders successful adoption or deployment of the product.\"\n",
      "}\n",
      "1\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"This entry is valid as it meets the criteria for Set 1 (Product Feedback and Limitations Criteria). It satisfies both A) Actionability and B) Specificity, as it mentions specific feedback about the Excel INDIRECT function and how it improves efficiency and speed. Additionally, it meets E) Positive Feedback by highlighting how the feature helps the user. Since it meets the requirements of Set 1, it is considered valid.\"\n",
      "}\n",
      "2\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The input provides actionable product feedback and identifies limitations of Microsoft Word web version and Microsoft Excel online version. The feedback is specific, mentioning that Word on the web feels clumsy and unclear, and Excel lacks a data analysis tool, pushing users to third-party tools. It also meets Set 1, Criteria A), B), and D) as it impacts customer experience. Additionally, the entry mentions a deployment blocker for Microsoft Word web version, citing visual presentation issues that prevent proper adoption. This meets Set 2 under Technical Barriers.\"\n",
      "}\n",
      "3\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The entry is considered valid as it meets criteria for both 'Feedback and limitations' and 'Deployment blockers.' It is valid for Set 1 because it meets criteria A) Actionability and B) Specificity as it provides actionable feedback about the resolution limitation and aligns with specific product features or limitations. Additionally, it meets criterion D) Impact on Customer Experience as the resolution incompatibility affects the customer's device usability. Regarding Set 2, it also meets the 'Compatibility' criterion because the resolution limitation explicitly mentions that it prevents the product‚Äôs use on the customer's specific device, which is a concrete issue of compatibility.\"\n",
      "}\n",
      "4\n",
      "{\n",
      "  \"valid\": false,\n",
      "  \"reasoning\": \"For Set 1 (Product Feedback and Limitations), the entry focuses on general positive feedback and broader benefits of Microsoft products without addressing specific product features or limitations in a manner that is actionable or valuable for a product team. It does not meet criteria A) and B). Although it aligns with customer objectives (C) and provides positive feedback (E), the lack of specificity and actionability makes it invalid. For Set 2 (Deployment Blockers), no details about deployment blockers have been provided and therefore does not meet any of the applicable criteria.\"\n",
      "}\n",
      "5\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The entry is valid under Set 1: Product Feedback and Limitations Criteria. It meets criteria A) and B) because it provides specific and actionable product feedback about the Outlook search feature not showing the folder where an email is located, which is valuable to the product team. It also meets criterion D) (Impact on Customer Experience) as this limitation could impact the workflow and efficiency of locating emails.\"\n",
      "}\n",
      "6\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The entry is valid as product feedback and limitations under Set 1. It meets criteria A) Actionability, since it identifies the lack of an option to arrange pages alphabetically in OneNote Web as a specific limitation. It also meets criteria B) Specificity, as it clearly refers to a particular feature limitation in the product (alphabetical arrangement of pages in the OneNote web app). It satisfies criteria D) Impact on Customer Experience, as it explains how the absence of this feature affects the customer's efficiency in managing notebooks. The deployment blockers portion is left undefined, so it's not assessed under Set 2.\"\n",
      "}\n",
      "7\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The entry meets the criteria for Set 1 (Product Feedback and Limitations). It clearly refers to a specific product feature (Microsoft Bookings and its seamless appointment scheduling experience), satisfying criteria B). It also provides positive feedback, mentioning that the customer finds the feature amazing, meeting criteria E).\"\n",
      "}\n",
      "8\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The entry is valid under Set 1 (Product Feedback and Limitations). It identifies a specific product limitation (difficulty in sending meeting invitations directly from Microsoft Teams), which meets criteria A (Actionability) and B (Specificity). Additionally, it impacts the customer experience by making the product harder to use, thus meeting criteria D (Impact on Customer Experience).\"\n",
      "}\n",
      "9\n",
      "{\n",
      "  \"valid\": false,\n",
      "  \"reasoning\": \"The entry doesn't meet the criteria for either Set 1 or Set 2. It mentions a random issue with OneDrive shared folders where files appear to be missing occasionally, but it does not provide specific actionable feedback for the product team (failing Set 1, Criteria A and B). Additionally, while the random bug affects productivity and reliability, it is too vague to qualify as a technical barrier or compatibility issue under Set 2 criteria.\"\n",
      "}\n",
      "10\n",
      "Rate limit is close, continuing in 60 seconds...\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The entry is considered valid because it meets criteria A) Actionability and B) Specificity from Set 1. It provides specific feedback about a limitation regarding routing calls from Auto Attendant to Non-Enterprise users, with an actionable suggestion related to licensing changes. Furthermore, it aligns with criterion C) Support of Objectives, as the feedback highlights the impact of extra licensing costs on the customer's operations, making it actionable and valuable for a product team to consider.\"\n",
      "}\n",
      "281.6364378929138\n",
      "  test_name  sensitivity  precision  f1_score\n",
      "0  original         0.50   0.909091  0.645161\n",
      "1  few shot         0.85   0.739130  0.790698\n"
     ]
    }
   ],
   "source": [
    "test_results = analyse_test_prompt('few shot', few_shot_prompt, test_results)\n",
    "\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95705f1f-feba-45d3-b837-32c092a1ccdd",
   "metadata": {},
   "source": [
    "Interestingly, the few shot prompt did not perform better than the few shot prompt on the cross validation set. I wonder if there's anything that could be done to optimise this "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31892e15-3d74-446a-beb9-c600ce7799c6",
   "metadata": {},
   "source": [
    "### 3.2 Multi-pass prompt\n",
    "\n",
    "A multi-pass prompt is a prompt engineering technique where an AI model is guided through multiple stages or iterations to refine its response. \n",
    "\n",
    "For this particular task, the product insights are considered valid whenever they are product feedback or deployment blockers - however, these have very different definitions.\n",
    "\n",
    "We could pass 2 different prompts to the model. \n",
    "\n",
    "1. We first check if it's product insight\n",
    "2. If it is, we're done, the entry is valid\n",
    "3. If not, we check if it's a deployment blocker\n",
    "4. If it is, the entry is valid\n",
    "5. If it's not, the entry is invalid.\n",
    "\n",
    "We're going to need a different method for analysing the prompt that implements this logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b6bddd8-8eb8-40d9-b879-1f2798aa6943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_multipass_prompt(test_name, prompt_product, prompt_deployment, results_store):\n",
    "    '''\n",
    "    Evaluates the performance of a prompt \n",
    "\n",
    "    Args:\n",
    "      - test_name: name of the test, can be used as an identifier\n",
    "      - prompt_product: system prompt passed to the LLM to validate product feedback\n",
    "      - prompt_deployment: system prompt passed to the LLM to validate deployment blockers\n",
    "      - results_store: dataframe where we can store the results\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    \n",
    "    dataframes = [\n",
    "        [true_positives, True], # first value contains the data, the second what we would like the model to return for every row\n",
    "        [true_negatives, False], # for instance, the llm should evaluate all true positives as valid to have 100% accuracy \n",
    "        [false_positives, False],\n",
    "        [false_negatives, True]\n",
    "    ]\n",
    "\n",
    "    # counters to evaluate metrics\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    llm_call_counter = 0\n",
    "    for dataframe in dataframes:\n",
    "        data, expectation = dataframe\n",
    "        \n",
    "        for index, row in data.iterrows():\n",
    "            # avoid token limit if needed every 10 rows \n",
    "            print(llm_call_counter)\n",
    "            if llm_call_counter >= 10:\n",
    "                print(f\"Rate limit is close, continuing in {60} seconds...\")\n",
    "                time.sleep(61)\n",
    "                llm_call_counter = 0\n",
    "                \n",
    "                \n",
    "            llm_res = send_prompt(prompt_product, row['Feedback'])\n",
    "            llm_res = clean_llm_response(llm_res)\n",
    "            llm_call_counter += 1\n",
    "\n",
    "            print(llm_res)\n",
    "            # if we did not get a TP or an TN, we use the other prompt\n",
    "            llm_res = json.loads(llm_res)\n",
    "            if expectation != llm_res['valid']:\n",
    "                llm_res = send_prompt(prompt_deployment, row['Feedback'])\n",
    "                llm_res = clean_llm_response(llm_res)\n",
    "                llm_call_counter += 1\n",
    "                \n",
    "            \n",
    "            \n",
    "            print(llm_res)\n",
    "            \n",
    "            llm_res = json.loads(llm_res)\n",
    "            if expectation == True and llm_res['valid'] == True:\n",
    "                tp += 1\n",
    "            elif expectation == True and llm_res['valid'] == False:\n",
    "                fn += 1\n",
    "            elif expectation == False and llm_res['valid'] == True:\n",
    "                fp += 1\n",
    "            elif expectation == False and llm_res['valid'] == False:\n",
    "                tn += 1\n",
    "\n",
    "    sensitivity = tp / ( tp + fn )\n",
    "    precision = tp / ( tp + fp )\n",
    "    f_1 = 2 * ( precision * sensitivity ) / ( precision + sensitivity )\n",
    "    new_results_row = pd.DataFrame({\"test_name\": [test_name] ,\"sensitivity\": [sensitivity] , \"precision\": [precision] , \"f1_score\": [f_1]  })\n",
    "\n",
    "    test_values = results_store[\"test_name\"].values\n",
    "    \n",
    "    if test_name in test_values:\n",
    "        index_to_replace = results_store[results_store[\"test_name\"] == test_name].index[0]\n",
    "        results_store.loc[index_to_replace] = new_results_row.iloc[0] \n",
    "    else:\n",
    "        results_store = pd.concat([results_store, new_results_row], ignore_index=True)\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time) \n",
    "    return results_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f22c1-7daa-478b-8ae4-e247dd972308",
   "metadata": {},
   "source": [
    "Now, let's break down the prompts with their examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99f83e93-7a87-4795-a9d7-6d308556475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_prompt = '''\n",
    "\n",
    "You are an AI assistant that validates entries based on specific criteria. \n",
    "\n",
    "Your job is to mark any entries given to you as valid or invalid. An input will be valid whenever it conforms to any of the following set of criteria.\n",
    "\n",
    "## Product Feedback and Limitations Criteria\n",
    "\n",
    "Meeting criteria A) and B) is a must have for the entry to be considered valid\n",
    "\n",
    "    - A) Actionability: the entry mentions product feedback, limitations that are specific, actionable and valuable for a product team.\n",
    "\n",
    "    - B) Specificity: The entry should clearly refer to a specific product feature or a product limitation\n",
    "    \n",
    "Meeting at least one of the following criteria C), D) and E) is enough to check the entry as valid\n",
    "\n",
    "    - C) Support of Objectives: The entry should explain how the feedback aligns customer business objectives or business case, regardless of whether the feedback is positive or negative. \n",
    "\n",
    "    - D) Impact on Customer Experience: The entry must explain how it impacts customer workflows, satisfaction, or any stage of the customer's experience\n",
    "    \n",
    "    - E) Positive Feedback: The entry provides positive feedback about a feature or aspect of the product\n",
    "\n",
    "##Criteria End##\n",
    "\n",
    "## Examples of valid Entries ## \n",
    "\n",
    "    - ‚ÄúThe Product seems very hard to use, doing basic actions like managing the calendar requires many clicks and it's confusing.‚Äù ‚Äì [ Valid, Meets Criteria A), B) and D) ]\n",
    "\n",
    "    - ‚ÄúThe Product is not able to perform scheduled updates, forcing the customer to do manual work and waste time‚Äù ‚Äì [ Valid, Meets Criteria A), B) and C) ]\n",
    "\n",
    "    - ‚ÄúThe customer mentioned how happy he was with the new data visualization suite and the FabricView feature. It has really helped his team make better decisions‚Äù ‚Äì [ Valid, Meets Criteria A), B) and E) ]\n",
    "\n",
    "    - ‚ÄúThe customer was frustrated because the product is unstable when running alongside another application‚Äù ‚Äì [ Valid - Meets criteria A) and B) and D) ]\n",
    "\n",
    "## Examples of valid Entries end ##\n",
    "\n",
    "## Examples of invalid Entries## \n",
    "\n",
    "    - ‚ÄúThe Product seems slow sometimes.‚Äù ‚Äì [ Invalid - Does not meet criteria A) and B) ]\n",
    "\n",
    "    - ‚ÄúThe customer does not like the product, he prefers the older version. Also thinks the competition is better‚Äù ‚Äì [ Invalid - Does not meet criteria A) and B) ]\n",
    "\n",
    "    - ‚ÄúWe heard from another company that they had issues with the product.‚Äù ‚Äì [ Invalid, Does not meet criteria A) and B) ]  \n",
    "\n",
    "    - ‚ÄúWe really love the product, the new functionalities are really cool and helps us make more money which is what we want‚Äù ‚Äì [ Invalid, Does not meet criteria A) and B) even though meets C) and E) ] \n",
    "\n",
    "    - ‚ÄúWe need time to adjust to new workflows.‚Äù - [ Invalid, Does not meet criteria A) and B) ]\n",
    "\n",
    "    - \"Product is a bit expensive, should rethink the price point\" - [ Invalid, Does not meet criteria A) and B) ]\n",
    "\n",
    "## Examples of invalid Entries for Set 1 end ##\n",
    "\n",
    "## Response ##\n",
    "\n",
    "You will always respond in JSON format with only the following fields and no more:\n",
    "\n",
    "* valid - make it true if the entry is considered valid, false if invalid\n",
    "* reasoning - add your reasoning based on the criteria set above\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af94e7f1-d8d8-4f46-bb72-5ef8f09a9860",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_blocker_prompt = '''\n",
    "You are an AI assistant that validates entries based on specific criteria. \n",
    "\n",
    "Your job is to mark any entries given to you as valid or invalid. An input will be valid whenever it conforms to any of the following set of criteria.\n",
    "\n",
    "##Valid Deployment Blockers Criteria## \n",
    "\n",
    "Meeting any of the following criteria is enough to check the entry as valid\n",
    "\n",
    "    - Technical Barriers: The entry contains concrete obstacles that prevents or limits the successful implementation, adoption, or performance of a technology, system or product.  \n",
    "\n",
    "    - Organizational Readiness: The entry refers to a shortage of trained personnel or expertise to adopt, implement or maintain a the product.\n",
    "\n",
    "    - Compatibility:  The entry explains clearly how the product cannot be adopted or used due to lack of compatibility, outdated systems, or proprietary formats \n",
    "\n",
    "    - Support and Documentation: The entry explains how poor documentation prevents the deployment, adoption or use of the product \n",
    "\n",
    "    - Security and Compliance: The entry explains risks related to data protection, cybersecurity threats, or compliance with privacy laws that prevent deployment, adoption or use of the product.\n",
    "\n",
    "## Valid Deployment Blockers Criteria end ## \n",
    "\n",
    "## Examples of valid Entries for Set 2 ## \n",
    "\n",
    "    - ‚ÄúThe customer raised a concern about the lack of multi-tenancy support. They need a way to manage multiple teams and departments separately within the product.‚Äù ‚Äì [ Valid, Technical Barrier ]\n",
    "\n",
    "    - ‚ÄúThe customer said that while they see the value in our solution, they can't deploy because their team would need extensive training to use it effectively.‚Äù ‚Äì [ Valid, Organizational Readiness ]\n",
    "\n",
    "    - ‚Äúhe customer mentioned that they were excited to deploy the product, but they discovered it's not compatible with their existing infrastructure. Their systems run on Linux, while the software only supports Windows, making it impossible for them to implement‚Äù ‚Äì [ Valid, Compatibility ]\n",
    "\n",
    "    - ‚ÄúThe customer said that when they encountered an issue, they couldn‚Äôt find sufficient troubleshooting guides or FAQs to resolve it on their own, making them overly reliant on support.‚Äù ‚Äì [ Valid, Support and Documentation ]\n",
    "\n",
    "    - \"The product only provides US-based data hosting but the customer requires GDPR, so legally they cannot use it.\" - [Valid, Security and Compliance]\n",
    "\n",
    "## Examples of valid Entries for Set 2 end ##\n",
    "\n",
    "## Examples of invalid Entries for Set 2 ## \n",
    "\n",
    "    - ‚ÄúOur office is moving next month, so we can‚Äôt focus on deployment right now.‚Äù ‚Äì [ Invalid, Does not meet any of the criteria ]\n",
    "\n",
    "    - ‚ÄúThe system doesn't seem to work as expected in our environment.‚Äù ‚Äì [ Invalid, Does not meet any of the criteria ]\n",
    "\n",
    "    - ‚ÄúThe customer mentioned that they are facing some challenges with the new system.‚Äù ‚Äì [ Invalid, Does not meet any of the criteria ]\n",
    "\n",
    "    - ‚ÄúThe customer said that they are not sure how to proceed with the migration.‚Äù ‚Äì [ Invalid, Does not meet any of the criteria ]\n",
    "\n",
    "    - \"They have concerns about security that need to be cleared before they proceed with the deployment\" - [Invalid, Does not meet any of the criteria]\n",
    "\n",
    "## Examples of valid Entries for Set 2 end ##\n",
    "\n",
    "## Response ##\n",
    "\n",
    "You will always respond in JSON format with only the following fields and no more:\n",
    "\n",
    "* valid - make it true if the entry is considered valid, false if invalid\n",
    "* reasoning - add your reasoning based on the criteria set above\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "202511a0-46a1-4382-9290-cf79d0a6fd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The entry meets criteria A) Actionability because it discusses specific, actionable product feedback. It meets B) Specificity because it clearly refers to the removal of an email encryption add-on as a limitation. It also meets D) Impact on Customer Experience because it explains how this change complicates the workflow and impacts secure communication, affecting customer satisfaction.\"\n",
      "}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[43manalyse_multipass_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmultipass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproduct_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeployment_blocker_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m test_results\n",
      "Cell \u001b[0;32mIn[37], line 46\u001b[0m, in \u001b[0;36manalyse_multipass_prompt\u001b[0;34m(test_name, prompt_product, prompt_deployment, results_store)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(llm_res)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# if we did not get a TP or an TN, we use the other prompt\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expectation \u001b[38;5;241m!=\u001b[39m \u001b[43mllm_res\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     47\u001b[0m     llm_res \u001b[38;5;241m=\u001b[39m send_prompt(prompt_deployment, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeedback\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     48\u001b[0m     llm_res \u001b[38;5;241m=\u001b[39m clean_llm_response(llm_res)\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "test_results = analyse_multipass_prompt('multipass', product_prompt, deployment_blocker_prompt, test_results)\n",
    "\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d22bd-8fa3-4f7a-a731-54e05ee1d680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
