{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503585de-7619-47c7-a627-872789c23f1e",
   "metadata": {},
   "source": [
    "# Product Insight Validation Using LLMs üîç\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebooks aims to evaluate different prompting strategies for validating product insights using a Large Language Model (LLM). The goal is to determine the most effective prompting approach for distinguishing between valid and invalid insights based on predefined criteria.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- **Compare Prompting Strategies:** Test multiple prompts and strategies to determine which yields the best classification results.\n",
    "- **Evaluate Performance:** Measure the effectiveness of each strategy using precision, recall, and F1 score.\n",
    "- **Cross-Validation Approach:** Utilize a labeled dataset containing:\n",
    "  - **True Positives (TP):** Correctly identified valid insights.\n",
    "  - **True Negatives (TN):** Correctly identified invalid insights.\n",
    "  - **False Positives (FP):** Incorrectly marked invalid insights as valid.\n",
    "  - **False Negatives (FN):** Incorrectly marked valid insights as invalid.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "1. **Load Product Insights**  \n",
    "   - Import CSV files containing product insights for validation.\n",
    "\n",
    "2. **Apply LLM-Based Validation**  \n",
    "   - Use different prompts and prompting strategies to classify insights.\n",
    "\n",
    "3. **Evaluate Performance**  \n",
    "   - Compute precision, recall, and F1 score to assess classification accuracy.\n",
    "   - Compare the effectiveness of different strategies based on their performance metrics.\n",
    "\n",
    "4. **Optimize for Accuracy**  \n",
    "   - Identify the best-performing prompt and strategy for product insight validation.\n",
    "\n",
    "## Tech Stack\n",
    "\n",
    "- **LLM Provider:** Azure OpenAI  \n",
    "- **Model:** ChatGPT 4.0  \n",
    "- **Data Processing:** Python (pandas, numpy)  \n",
    "- **Evaluation Metrics:** precision, recall, F1 score  \n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "- A clear understanding of which prompting strategy yields the best results.\n",
    "- A methodology/workflow that can be iteratively improved and scaled for future product insight validation tasks.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "_This notebook has memes every once in a while. Jupyter notebooks are very nice but also can be a bit dry. The memes are not particularly good, don't judge me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6a5a9dc-0cbe-4f4c-bb91-51722615fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import the packages we will need for this project\n",
    "\n",
    "import requests # for connecting with Azure Open AI\n",
    "import json # for parsing responses\n",
    "import csv # for data processing\n",
    "import pandas as pd # for data analysis \n",
    "\n",
    "# let's also import the config we will need to interact with the Azure Open AI API\n",
    "\n",
    "from config import config_endpoint, config_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac873b2-0cbe-4539-ab03-41b717a6eeab",
   "metadata": {},
   "source": [
    "# 1 - Load Product Insights\n",
    "\n",
    "Let's take a glimpse at the data we have. All this data has been validated with an LLM with a custom prompt and then reviewed by human validators. This explains why we have true and false positives and negatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea37f835-c3be-426d-832a-f2891a6e5769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feedback</th>\n",
       "      <th>Product Feedback and Limitations validation_status</th>\n",
       "      <th>Product Feedback and Limitations comment</th>\n",
       "      <th>Product Feedback and Limitations_human_review</th>\n",
       "      <th>Product Feedback and Limitations_human_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feedback and limitations - **Details ** Custom...</td>\n",
       "      <td>1</td>\n",
       "      <td>The feedback is valid as it specifically addre...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Valid concern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feedback and limitations Product Limitation \\n...</td>\n",
       "      <td>1</td>\n",
       "      <td>The feedback is specific as it refers to the d...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feedback and limitations Customer appreciates ...</td>\n",
       "      <td>1</td>\n",
       "      <td>The feedback is specific, mentioning the centr...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Specific, actionable product feedback with cle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feedback and limitations Customer have mention...</td>\n",
       "      <td>1</td>\n",
       "      <td>The feedback is specific, mentioning the activ...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feedback and limitations   After deleting the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>The feedback is specific, mentioning the issue...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Feedback  \\\n",
       "0  Feedback and limitations - **Details ** Custom...   \n",
       "1  Feedback and limitations Product Limitation \\n...   \n",
       "2  Feedback and limitations Customer appreciates ...   \n",
       "3  Feedback and limitations Customer have mention...   \n",
       "4  Feedback and limitations   After deleting the ...   \n",
       "\n",
       "   Product Feedback and Limitations validation_status  \\\n",
       "0                                                  1    \n",
       "1                                                  1    \n",
       "2                                                  1    \n",
       "3                                                  1    \n",
       "4                                                  1    \n",
       "\n",
       "            Product Feedback and Limitations comment  \\\n",
       "0  The feedback is valid as it specifically addre...   \n",
       "1  The feedback is specific as it refers to the d...   \n",
       "2  The feedback is specific, mentioning the centr...   \n",
       "3  The feedback is specific, mentioning the activ...   \n",
       "4  The feedback is specific, mentioning the issue...   \n",
       "\n",
       "  Product Feedback and Limitations_human_review  \\\n",
       "0                                         Agree   \n",
       "1                                         Agree   \n",
       "2                                         Agree   \n",
       "3                                         Agree   \n",
       "4                                         Agree   \n",
       "\n",
       "      Product Feedback and Limitations_human_comment  \n",
       "0                                      Valid concern  \n",
       "1                                                NaN  \n",
       "2  Specific, actionable product feedback with cle...  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We load all the data \n",
    "\n",
    "true_positives = pd.read_csv('true_positive_sample.csv')\n",
    "true_negatives = pd.read_csv('true_negative_sample.csv')\n",
    "false_positives = pd.read_csv('false_positive_sample.csv')\n",
    "false_negatives = pd.read_csv('false_negative_sample.csv')\n",
    "\n",
    "# Now let's print one of the datasets to see its shape\n",
    "\n",
    "true_positives[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a622af27-5a45-4d55-92be-76c5b75a9a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column Name</th>\n",
       "      <th>Explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feedback</td>\n",
       "      <td>Raw feedback notes captured by the agent and stored on Gigplus Trackers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product Feedback and Limitations validation_status</td>\n",
       "      <td>Validation done by the LLM - 0 is invalid, 1 is valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Product Feedback and Limitations comment</td>\n",
       "      <td>Explanation provided by the LLM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Product Feedback and Limitations_human_review</td>\n",
       "      <td>Human review, agreeing or disagreeing with the model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product Feedback and Limitations_human_comment</td>\n",
       "      <td>Comment left by the human validator</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Column Name  \\\n",
       "0                                            Feedback   \n",
       "1  Product Feedback and Limitations validation_status   \n",
       "2            Product Feedback and Limitations comment   \n",
       "3       Product Feedback and Limitations_human_review   \n",
       "4      Product Feedback and Limitations_human_comment   \n",
       "\n",
       "                                                               Explanation  \n",
       "0  Raw feedback notes captured by the agent and stored on Gigplus Trackers  \n",
       "1                    Validation done by the LLM - 0 is invalid, 1 is valid  \n",
       "2                                          Explanation provided by the LLM  \n",
       "3                     Human review, agreeing or disagreeing with the model  \n",
       "4                                      Comment left by the human validator  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column explanation\n",
    "data = [\n",
    "    [\"Feedback\", \"Raw feedback notes captured by the agent and stored on Gigplus Trackers\"],\n",
    "    [\"Product Feedback and Limitations validation_status\", \"Validation done by the LLM - 0 is invalid, 1 is valid\"],\n",
    "    [\"Product Feedback and Limitations comment\", \"Explanation provided by the LLM\"],\n",
    "    [\"Product Feedback and Limitations_human_review\", \"Human review, agreeing or disagreeing with the model\"],\n",
    "    [\"Product Feedback and Limitations_human_comment\", \"Comment left by the human validator\"]\n",
    "]\n",
    "column_data = pd.DataFrame(data, columns=[\"Column Name\", \"Explanation\"])\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None) \n",
    "column_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ffb247-9a9f-4505-8b68-10aae8ac23a8",
   "metadata": {},
   "source": [
    "## 1.1 Baselining Performance\n",
    "\n",
    "Let's calculate Sensitivity, Recall and F1 for this dataset, which will give us target performance metrics to iterate on. Let's refresh on how these are calculated\n",
    "\n",
    "# Performance Metrics\n",
    "\n",
    "## Sensitivity (Recall)\n",
    "Sensitivity, also known as **recall**, measures the ability to correctly identify positive cases:\n",
    "\n",
    "$$\n",
    "\\text{Sensitivity} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}\n",
    "$$\n",
    "\n",
    "## Precision\n",
    "Precision measures how many of the predicted positive cases were actually correct:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}\n",
    "$$\n",
    "\n",
    "## F1 Score\n",
    "F1 Score is the harmonic mean of precision and recall, balancing both metrics:\n",
    "\n",
    "$$\n",
    "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Sensitivity}}{\\text{Precision} + \\text{Sensitivity}}\n",
    "$$\n",
    "\n",
    "\n",
    "With that, let's calculate sensitivity, precision and F1 score for our current dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "237f8dc1-de9e-4064-969c-74312b37e9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F1_Score</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Metric     Value\n",
       "0  Sensitivity  0.571429\n",
       "1    Precision  0.666667\n",
       "2     F1_Score  0.615385"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp = true_positives.shape[0]  \n",
    "tn = true_negatives.shape[0]\n",
    "fp = false_positives.shape[0]\n",
    "fn = false_negatives.shape[0]\n",
    "\n",
    "sensitivity = tp / ( tp + fn )\n",
    "precision = tp / ( tp + fp )\n",
    "f_1 = 2 * ( precision * sensitivity ) / ( precision + sensitivity )\n",
    "\n",
    "baseline_eval_metrics = pd.DataFrame([\n",
    "    [\"Sensitivity\", sensitivity],\n",
    "    [\"Precision\", precision],\n",
    "    [\"F1_Score\", f_1]\n",
    "    ],\n",
    "    columns=[\"Metric\", \"Value\"]\n",
    ")\n",
    "\n",
    "baseline_eval_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79441836-5541-4417-954a-f158c16499da",
   "metadata": {},
   "source": [
    "The metrics are very low and in principle \"easy to beat\", but this is only because the sample size is very small for true positives and true negatives. \n",
    "\n",
    "In reality, the previous model performed better than this - nevertheless, this gives us a compass for our exercise.\n",
    "\n",
    "**New prompts/prompt strategies should be able to have a better ability to catch false positives and false negatives while maintaining accuracy with true positives and negatives**\n",
    "\n",
    "We'll store the result of all our tests into a dataframe table. This will allow us to contrast and compare approaches and make a final selection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe892272-381c-41f5-af17-08e06550115f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xv/5lp8ff8s7j55zh1lrpr9ddkw0000gn/T/ipykernel_16247/2216797742.py:3: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_results = pd.concat([test_results, pd.DataFrame({\"test_name\": [\"original\"] ,\"sensivity\": [sensitivity] , \"precision\": [precision] , \"f1_score\": [f_1]  })], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_name</th>\n",
       "      <th>sensivity</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>original</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  test_name  sensivity  precision  f1_score\n",
       "0  original   0.571429   0.666667  0.615385"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results = pd.DataFrame([], columns=[\"test_name\", \"sensivity\", \"precision\", \"f1_score\"])\n",
    "\n",
    "test_results = pd.concat([test_results, pd.DataFrame({\"test_name\": [\"original\"] ,\"sensivity\": [sensitivity] , \"precision\": [precision] , \"f1_score\": [f_1]  })], ignore_index=True)\n",
    "\n",
    "test_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb9dfaf-80e7-4e4d-8eeb-ab90d40ec221",
   "metadata": {},
   "source": [
    "## 2. Apply LLM Validation\n",
    "\n",
    "Let's start this section by defining a function that calls Azure Open AI with a system prompt, and an input provided by the user. \n",
    "\n",
    "The system prompt will contain the criteria to validate an insight, and the user input will be the entry registered by our agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9b8fab4b-9724-4a7d-80c7-6258db4c9e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": config_key\n",
    "}\n",
    "\n",
    "def send_prompt(system_prompt, user_prompt, max_tokens=200):\n",
    "    \"\"\"Send a prompt to Azure OpenAI and return the response.\"\"\"\n",
    "    url = config_endpoint\n",
    "    data = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=HEADERS, json=data)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb52e7-0ebb-4809-ace7-ca6a43ab23cb",
   "metadata": {},
   "source": [
    "Let's test it out with a very naive example to make sure it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6b865821-a50d-443f-88b3-48ecdccea9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reason\": \"The feedback highlights a usability issue regarding the app's menu being convoluted and crowded with icons, which makes it hard to use and read. This directly relates to how users interact with the app.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "res = send_prompt(\n",
    "    \"You are system dedicated to validate product feedback. You will only declare as valid feedback that has to do usability issues, anything else will be invalid. Always return json with two fields: { valid: can only be true or false, reason: your reasoning as to why the insight is valid or invalid }\",\n",
    "    \"I could not use the app at all, the menu was very convoluted and crowded with icons. Very hard to read\"\n",
    ")\n",
    "\n",
    "print(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a825f309-c7c8-4ebf-8cf3-41f5c051eaf1",
   "metadata": {},
   "source": [
    "The model is giving us back a string formatted in Markdown. Let's create a function to clean it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c839bbdc-1bac-4670-bce4-4eafbdd429ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_llm_response(res):\n",
    "    return res.replace(\"json\", \"\").replace(r'\\n', '').replace(r\"\\'\", \"'\").replace(\"`\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "230f7637-2091-4ac8-bf78-a97087ec0f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"valid\": true,\n",
      "  \"reason\": \"The feedback highlights a usability issue regarding the app's menu being convoluted and crowded with icons, which makes it hard to use and read. This directly relates to how users interact with the app.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(clean_llm_response(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e3e61-d54e-4363-a040-2d5ddb7f12b1",
   "metadata": {},
   "source": [
    "Great! We now have the basic building block for testing different validation prompts.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049ff6ea-b393-48bd-a11b-f460424eadad",
   "metadata": {},
   "source": [
    "## 3. Analysing prompting approaches\n",
    "\n",
    "Now we need a function that allows us to do the following:\n",
    "\n",
    "- 1. Iterate through our TP, TN, FP, FN datasets.\n",
    "- 2. For each of the rows\n",
    "    - 1. Ask the LLM to validate  \n",
    "    - 2. Evaluate if the LLM did a good job or not\n",
    "    - 3. Store this information somewhere\n",
    "- 4. Calculate Sensitivity, Recall and F1\n",
    "- 5. Add the results to our run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09ae07-7b05-47d6-ba7d-bd220aab3891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
