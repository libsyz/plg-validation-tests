{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503585de-7619-47c7-a627-872789c23f1e",
   "metadata": {},
   "source": [
    "# Product Insight Validation Using LLMs: M365 Product Feedbacküîç\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebooks aims to evaluate different prompting strategies and prompts for validating M365 Product Feedback using a Large Language Model (LLM). The goal is to have a sandbox where we can fine-tune prompts using the same cross validation sets. \n",
    "\n",
    "## Objectives\n",
    "\n",
    "- **Compare Prompting Strategies:** Test multiple prompts and strategies to determine which yields the best classification results.\n",
    "- **Evaluate Performance:** Measure the effectiveness of each strategy using precision, recall, and F1 score.\n",
    "- **Cross-Validation Approach:** Utilize a labeled dataset containing:\n",
    "  - **True Positives (TP):** Correctly identified valid insights.\n",
    "  - **True Negatives (TN):** Correctly identified invalid insights.\n",
    "  - **False Positives (FP):** Incorrectly marked invalid insights as valid.\n",
    "  - **False Negatives (FN):** Incorrectly marked valid insights as invalid.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "1. **Load Product Insights**  \n",
    "   - Import CSV files containing business goals for validation.\n",
    "\n",
    "2. **Apply LLM-Based Validation**  \n",
    "   - Building blocks for using LLM to validate, and cleaning inputs\n",
    "\n",
    "3. **Evaluate Performance**  \n",
    "   - Compute precision, recall, and F1 score to assess classification accuracy.\n",
    "   - Compare the effectiveness of different strategies based on their performance metrics.\n",
    "   - 3.1 Zero Shot Prompting\n",
    "   - 3.2 Few Shot Prompting\n",
    "   - 3.3 Multi-pass w/ Few Shot prompting\n",
    "\n",
    "4. **Optimize for Accuracy**  \n",
    "   - Identify the best-performing prompt and strategy for product insight validation.\n",
    "\n",
    "## Tech Stack\n",
    "\n",
    "- **LLM Provider:** Azure OpenAI  \n",
    "- **Model:** ChatGPT 4.0  \n",
    "- **Data Processing:** Python (pandas, numpy)  \n",
    "- **Evaluation Metrics:** precision, recall, F1 score  \n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "- A clear understanding of which prompting strategy yields the best results.\n",
    "- A methodology/workflow that can be iteratively improved and scaled for future product insight validation tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6a5a9dc-0cbe-4f4c-bb91-51722615fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import the packages we will need for this project\n",
    "\n",
    "import requests # for connecting with Azure Open AI\n",
    "import json # for parsing responses\n",
    "import csv # for data processing\n",
    "import pandas as pd # for data analysis \n",
    "\n",
    "# let's also import the config we will need to interact with the Azure Open AI API\n",
    "\n",
    "from config import config_endpoint, config_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac873b2-0cbe-4539-ab03-41b717a6eeab",
   "metadata": {},
   "source": [
    "# 1 - Load M365 Product Feedback\n",
    "\n",
    "Let's take a glimpse at the data we have. All these business goals have been validated by human validators during the month of April '25 \n",
    "\n",
    "These datasets will act as makeshift cross-validation sets, that we can use to test the performance of different prompting strategies and approaches. \n",
    "\n",
    "\n",
    "We have two datasets:\n",
    "\n",
    "- A small set of 20 cases ( 10 valid, 10 invalids ) for quick experimentation\n",
    "- A bigger set of 200 cases ( 100 valid, 100 invalids ) for testing and measuring results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea37f835-c3be-426d-832a-f2891a6e5769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Case Number</th>\n",
       "      <th>UPN</th>\n",
       "      <th>Line Of Business</th>\n",
       "      <th>Product Feedback and Limitations</th>\n",
       "      <th>Product Feedback and Limitations Validation</th>\n",
       "      <th>Product Feedback and Limitations Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27636</td>\n",
       "      <td>2504050040000885</td>\n",
       "      <td>gig_wfh_udmem@office365support.com</td>\n",
       "      <td>Business Advisor Reactive</td>\n",
       "      <td>\\nCustomer complained that the domain verification is for the users with have IT skills, and not al of the users are , Microsoft should create a system when adding the domain to the Microsoft 365 will not have to be manually, it it can be done by just clicking on a button so that the domain can be looked into.</td>\n",
       "      <td>1</td>\n",
       "      <td>Feedback is valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25781</td>\n",
       "      <td>2504010040001913</td>\n",
       "      <td>gig_wfh_alass@microsoftsupport.com</td>\n",
       "      <td>Proactive Grace</td>\n",
       "      <td>Feedback and limitations: The customer noted that the admin portal is vague and not straightforward because it is hard to find what the customer's looking for in the long menus. The customer justified his feedback by a situation he experienced which that he has been trying for 2 years to change and cease the subscription renewal cycle from the billing section in the portal but couldn't figure it out himself. \\n\\nProduct Feedback and Limitations:</td>\n",
       "      <td>1</td>\n",
       "      <td>Valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27911</td>\n",
       "      <td>2503310010001432</td>\n",
       "      <td>gig_wfh_hariv@microsoftsupport.com</td>\n",
       "      <td>Business Assist</td>\n",
       "      <td>\\nThe customer seeks to restrict entry-level employees' usage of M365 resources and control their access without subscribing to additional add-in licenses. They require a solution similar to Apple Business Manager, which allows them to limit device functionality and user experience without incurring extra costs for user-based licenses, focusing solely on device subscriptions.</td>\n",
       "      <td>1</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26153</td>\n",
       "      <td>2504020030001649</td>\n",
       "      <td>gig_wfh_jibal@microsoftsupport.com</td>\n",
       "      <td>Business Advisor Reactive</td>\n",
       "      <td>M365 Product Feedback: \\nIt would be beneficial for the customer if, when verifying their business domain from a third-party domain host, Microsoft would simply ask for a sign-in page. This would make it easier to add the DNS records instantly and save time on verification.</td>\n",
       "      <td>1</td>\n",
       "      <td>Valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26494</td>\n",
       "      <td>2503310040002168</td>\n",
       "      <td>gig_wfh_avadv@microsoftsupport.com</td>\n",
       "      <td>Trials Nurturing Proactive</td>\n",
       "      <td>Feedback and limitations: The customer emphasized that integrating Forms and SharePoint for appointment scheduling greatly enhances their shop's operations. Forms collects detailed booking information accurately, while SharePoint securely organizes data for staff access and scheduling. They particularly value the simplicity of using a QR code to direct clients to the system, streamlining the process and improving overall efficiency.\\n\\nProduct Feedback and Limitations:</td>\n",
       "      <td>1</td>\n",
       "      <td>Valid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID       Case Number                                 UPN  \\\n",
       "0  27636  2504050040000885  gig_wfh_udmem@office365support.com   \n",
       "1  25781  2504010040001913  gig_wfh_alass@microsoftsupport.com   \n",
       "2  27911  2503310010001432  gig_wfh_hariv@microsoftsupport.com   \n",
       "3  26153  2504020030001649  gig_wfh_jibal@microsoftsupport.com   \n",
       "4  26494  2503310040002168  gig_wfh_avadv@microsoftsupport.com   \n",
       "\n",
       "             Line Of Business  \\\n",
       "0   Business Advisor Reactive   \n",
       "1             Proactive Grace   \n",
       "2             Business Assist   \n",
       "3   Business Advisor Reactive   \n",
       "4  Trials Nurturing Proactive   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                             Product Feedback and Limitations  \\\n",
       "0                                                                                                                                                                     \\nCustomer complained that the domain verification is for the users with have IT skills, and not al of the users are , Microsoft should create a system when adding the domain to the Microsoft 365 will not have to be manually, it it can be done by just clicking on a button so that the domain can be looked into.   \n",
       "1                          Feedback and limitations: The customer noted that the admin portal is vague and not straightforward because it is hard to find what the customer's looking for in the long menus. The customer justified his feedback by a situation he experienced which that he has been trying for 2 years to change and cease the subscription renewal cycle from the billing section in the portal but couldn't figure it out himself. \\n\\nProduct Feedback and Limitations:    \n",
       "2                                                                                                  \\nThe customer seeks to restrict entry-level employees' usage of M365 resources and control their access without subscribing to additional add-in licenses. They require a solution similar to Apple Business Manager, which allows them to limit device functionality and user experience without incurring extra costs for user-based licenses, focusing solely on device subscriptions.   \n",
       "3                                                                                                                                                                                                          M365 Product Feedback: \\nIt would be beneficial for the customer if, when verifying their business domain from a third-party domain host, Microsoft would simply ask for a sign-in page. This would make it easier to add the DNS records instantly and save time on verification.   \n",
       "4  Feedback and limitations: The customer emphasized that integrating Forms and SharePoint for appointment scheduling greatly enhances their shop's operations. Forms collects detailed booking information accurately, while SharePoint securely organizes data for staff access and scheduling. They particularly value the simplicity of using a QR code to direct clients to the system, streamlining the process and improving overall efficiency.\\n\\nProduct Feedback and Limitations:    \n",
       "\n",
       "   Product Feedback and Limitations Validation  \\\n",
       "0                                            1   \n",
       "1                                            1   \n",
       "2                                            1   \n",
       "3                                            1   \n",
       "4                                            1   \n",
       "\n",
       "  Product Feedback and Limitations Comment  \n",
       "0                        Feedback is valid  \n",
       "1                                    Valid  \n",
       "2                                    valid  \n",
       "3                                    Valid  \n",
       "4                                    Valid  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We load all the data \n",
    "\n",
    "m365_product_feedback_20 = pd.read_csv('./cases/m365_feedback_cv20.csv') # 2-3 min\n",
    "m365_product_feedback_200 = pd.read_csv('./cases/m365_feedback_cv200.csv') # 25-30 min\n",
    "\n",
    "# Now let's print one of the datasets to see its shape\n",
    "\n",
    "m365_product_feedback_20[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a622af27-5a45-4d55-92be-76c5b75a9a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column Name</th>\n",
       "      <th>Explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Product Feedback and Limitations</td>\n",
       "      <td>Raw M365 product feedback captured by the ambassador as they figure in the tracker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product Feedback and Limitations Validation</td>\n",
       "      <td>Validation done by a human validator - 0 is invalid, 1 is valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Product Feedback and Limitations Comment</td>\n",
       "      <td>Comment/Explanation provided by a human validator</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Column Name  \\\n",
       "0             Product Feedback and Limitations   \n",
       "1  Product Feedback and Limitations Validation   \n",
       "2     Product Feedback and Limitations Comment   \n",
       "\n",
       "                                                                          Explanation  \n",
       "0  Raw M365 product feedback captured by the ambassador as they figure in the tracker  \n",
       "1                     Validation done by a human validator - 0 is invalid, 1 is valid  \n",
       "2                                   Comment/Explanation provided by a human validator  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column explanation\n",
    "data = [\n",
    "    [\"Product Feedback and Limitations\", \"Raw M365 product feedback captured by the ambassador as they figure in the tracker\"],\n",
    "    [\"Product Feedback and Limitations Validation\", \"Validation done by a human validator - 0 is invalid, 1 is valid\"],\n",
    "    [\"Product Feedback and Limitations Comment\", \"Comment/Explanation provided by a human validator\"]\n",
    "]\n",
    "column_data = pd.DataFrame(data, columns=[\"Column Name\", \"Explanation\"])\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None) \n",
    "column_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ffb247-9a9f-4505-8b68-10aae8ac23a8",
   "metadata": {},
   "source": [
    "## 1.1 Defining Performance\n",
    "\n",
    "These 2 datasets have already been evaluated by human validators.\n",
    "\n",
    "This means we can use the previous labels to calculate Sensitivity, Recall and F1 for this dataset, which will give us performance metrics we can analyse and optimise. We will be looking at the following metrics.\n",
    "\n",
    "# Performance Metrics\n",
    "\n",
    "## Sensitivity (Recall)\n",
    "Sensitivity, also known as **recall**, measures the ability to correctly identify positive cases:\n",
    "\n",
    "$$\n",
    "\\text{Sensitivity} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}\n",
    "$$\n",
    "\n",
    "## Precision\n",
    "Precision measures how many of the predicted positive cases were actually correct:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}\n",
    "$$\n",
    "\n",
    "## F1 Score\n",
    "F1 Score is the harmonic mean of precision and recall, balancing both metrics:\n",
    "\n",
    "$$\n",
    "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Sensitivity}}{\\text{Precision} + \\text{Sensitivity}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45295006-bd78-4f74-aae1-dc3509c24e29",
   "metadata": {},
   "source": [
    "We will create a dataframe where we will store the results of our tests as we run them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe892272-381c-41f5-af17-08e06550115f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_name</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [test_name, sensitivity, precision, f1_score]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results = pd.DataFrame([], columns=[\"test_name\", \"sensitivity\", \"precision\", \"f1_score\"])\n",
    "\n",
    "\n",
    "test_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb9dfaf-80e7-4e4d-8eeb-ab90d40ec221",
   "metadata": {},
   "source": [
    "## 2. Setting Up Logic for LLM Validation and Analysis\n",
    "\n",
    "### 2.1 Validation\n",
    "\n",
    "Let's start this section by defining a function that calls Azure Open AI with a system prompt, and an input provided by the user. \n",
    "\n",
    "The system prompt will contain the criteria to validate an insight, and the user input will be the entry registered by our agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b8fab4b-9724-4a7d-80c7-6258db4c9e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": config_key\n",
    "}\n",
    "\n",
    "def send_prompt(system_prompt, user_prompt, max_tokens=200):\n",
    "    \"\"\"Send a prompt to Azure OpenAI and return the response.\"\"\"\n",
    "    url = config_endpoint\n",
    "    data = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=HEADERS, json=data)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb52e7-0ebb-4809-ace7-ca6a43ab23cb",
   "metadata": {},
   "source": [
    "Let's test it out with a very naive example to make sure it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b865821-a50d-443f-88b3-48ecdccea9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reason\": \"The feedback highlights a usability issue where the menu design is convoluted and overcrowded, leading to difficulties in navigation and readability. This directly affects the user's ability to use the app effectively.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "res = send_prompt(\n",
    "    \"You are system dedicated to validate product feedback. You will only declare as valid feedback that has to do usability issues, anything else will be invalid. Always return json with two fields: { valid: can only be true or false, reason: your reasoning as to why the insight is valid or invalid }\",\n",
    "    \"I could not use the app at all, the menu was very convoluted and crowded with icons. Very hard to read\"\n",
    ")\n",
    "\n",
    "print(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a825f309-c7c8-4ebf-8cf3-41f5c051eaf1",
   "metadata": {},
   "source": [
    "The model is giving us back a JSON wrapped in Markdown. Let's create a function to clean it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c839bbdc-1bac-4670-bce4-4eafbdd429ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_llm_response(res):\n",
    "    return res.replace(\"json\", \"\").replace(r'\\n', '').replace(r\"\\'\", \"'\").replace(\"`\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "230f7637-2091-4ac8-bf78-a97087ec0f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"valid\": true,\n",
      "  \"reason\": \"The feedback highlights a usability issue where the menu design is convoluted and overcrowded, leading to difficulties in navigation and readability. This directly affects the user's ability to use the app effectively.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(clean_llm_response(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e3e61-d54e-4363-a040-2d5ddb7f12b1",
   "metadata": {},
   "source": [
    "Great! We now have the basic building block for testing different validation prompts.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049ff6ea-b393-48bd-a11b-f460424eadad",
   "metadata": {},
   "source": [
    "###  2.2 Analysis of prompt performance\n",
    "\n",
    "Now we need a function that allows us to do the following:\n",
    "\n",
    "- 1. Iterate through the rows of one of our datasets.\n",
    "- 2. For each of the rows in each of the datasets\n",
    "    - 1. Ask the LLM to validate the entry\n",
    "    - 2. Evaluate if the LLM did a good job or not\n",
    "          - LLM => Valid, Human => Valid, then *true positive*\n",
    "          - LLM => Invalid, Human => Invalid, then *true negative*\n",
    "          - LLM => Valid, Human => Invalid, then *false positive*\n",
    "          - LLM => Invalid, Human => Valid, then *false negative*\n",
    "    - 3. Store this information\n",
    "- 4. Calculate Sensitivity, Recall and F1 for this prompt\n",
    "- 5. Add the results to our log in the `test_results` variable we created before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d09ae07-7b05-47d6-ba7d-bd220aab3891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import ipdb;\n",
    "\n",
    "def analyse_test_prompt(test_name, prompt, results_store, dataset):\n",
    "    '''\n",
    "    Evaluates the performance of a prompt \n",
    "\n",
    "    Args:\n",
    "      - test_name: name of the test, can be used as an identifier\n",
    "      - prompt: system prompt passed to the LLM to validate product feedback\n",
    "      - results_store: dataframe where we can store the results\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "\n",
    "    # counters to evaluate metrics\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    row_counter = 0\n",
    "    for index, row in dataset.iterrows():\n",
    "        # avoid token limit if needed every 10 rows \n",
    "        print(row_counter)\n",
    "        if row_counter >= 10:\n",
    "            print(f\"Rate limit is close, continuing in {60} seconds...\")\n",
    "            time.sleep(61)\n",
    "            row_counter = 0\n",
    "                \n",
    "        llm_res = send_prompt(prompt, row['Product Led Growth Conversation'])\n",
    "        llm_res = clean_llm_response(llm_res)\n",
    "        row_counter += 1\n",
    "        print(row['Product Led Growth Conversation'],llm_res)\n",
    "\n",
    "        try:\n",
    "            llm_res = json.loads(llm_res)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"[WARN] Failed to parse LLM response as JSON: {e}\")\n",
    "            continue\n",
    "\n",
    "        human_validation = row['PLG Conversation Validation']\n",
    "        if llm_res['valid'] == True and human_validation == 1:\n",
    "            tp += 1\n",
    "        elif llm_res['valid'] == False and human_validation == 0:\n",
    "            tn += 1\n",
    "        elif llm_res['valid'] == True and human_validation == 0:\n",
    "            fp += 1\n",
    "        elif llm_res['valid'] == False and human_validation == 1:\n",
    "            tn += 1\n",
    "\n",
    "    sensitivity = tp / ( tp + fn )\n",
    "    precision = tp / ( tp + fp )\n",
    "    f_1 = 2 * ( precision * sensitivity ) / ( precision + sensitivity )\n",
    "    \n",
    "    new_results_row = pd.DataFrame({\"test_name\": [test_name] ,\"sensitivity\": [sensitivity] , \"precision\": [precision] , \"f1_score\": [f_1]  })\n",
    "\n",
    "    test_values = results_store[\"test_name\"].values\n",
    "    \n",
    "    if test_name in test_values:\n",
    "        index_to_replace = results_store[results_store[\"test_name\"] == test_name].index[0]\n",
    "        results_store.loc[index_to_replace] = new_results_row.iloc[0] \n",
    "    else:\n",
    "        results_store = pd.concat([results_store, new_results_row], ignore_index=True)\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time) \n",
    "    return results_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc35f49-c179-43b4-a4e9-53e448dade76",
   "metadata": {},
   "source": [
    "Ok, our building blocks of logic are now ready. Let's start with some prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1390c5-acb7-433a-a7e7-fd23476370bb",
   "metadata": {},
   "source": [
    "## 3. Testing Prompting Approaches\n",
    "\n",
    "In this section we will test the performance of several prompting approaches to see which one seems performs better. Let's go!\n",
    "\n",
    "\n",
    "### 3.1 Loose Zero Shot Prompt\n",
    "\n",
    "Zero-shot prompting is a technique used with large language models (LLMs) where the model is asked to perform a task without being given any specific examples of how to do it. We're relying entirely on the model's pre-existing knowledge and understanding to generate a response. ¬†\n",
    "\n",
    "In the prompt below, we describe high level criteria that is frequently mentioned by human validators to mark insights as valid or invalid. These are drawn from analysing the reasons as to why validators mark insights as valid or invalid. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "063cf567-e259-4371-a948-ce87cac5485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_zero_shot_prompt = '''\n",
    "You are an AI assistant that validates entries based on specific criteria. \n",
    "\n",
    "Your job is to mark any entries given to you as valid or invalid. An input will be valid whenever it conforms to any of the following sets of criteria.\n",
    "\n",
    "## Set 1: Valid Business Goals Criteria\n",
    "\n",
    "Meeting all criteria below is a must have for the entry to be considered valid, otherwise it will be invalid\n",
    "\n",
    "    - A) Clarity: the entry mentions a business goal or need that is clear and easy to understand\n",
    "\n",
    "    - B) Specificity: The entry should clearly refer to a specific, concrete business goal or need\n",
    "\n",
    "    - C) Actionable: The entry should focus on the practical applicability of Microsoft 365 products to address the business need described\n",
    "\n",
    "## Set 2: Invalid Business Goal Criteria\n",
    "\n",
    "Meeting any of the criteria below is enough for the entry to be considered invalid.\n",
    "\n",
    "    - D) Focus on tools: The entry just lists the M365 applications being used, but there is no business goal or need mentioned\n",
    "\n",
    "    - E) Vague business goal or need: The entry does not include any details nor actionable business goals/needs\n",
    "\n",
    "    - F) Technical issue: The entry only describes a technical issue experienced by the customer and there is no business goal or need\n",
    "\n",
    "## Response ##\n",
    "\n",
    "You will always respond in JSON format with the following fields:\n",
    "* valid - make it true if the entry is considered valid, false if invalid\n",
    "* reasoning - add your reasoning based on the criteria set above\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abe810ab-1123-4077-8a2f-e67920ad6d91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test Loose Zero Shot Prompt here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "781d156d-5afc-4074-a6be-456de0aac464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_name</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loose_zero_shot_20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>loose_zero_shot_20_no_negative_criteria</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>loose_zero_shot_200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.837209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 test_name  sensitivity  precision  f1_score\n",
       "0                       loose_zero_shot_20          1.0   0.615385  0.761905\n",
       "1  loose_zero_shot_20_no_negative_criteria          1.0   0.642857  0.782609\n",
       "2                      loose_zero_shot_200          1.0   0.720000  0.837209"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f8bcf6-2c75-4fc9-8741-c985fe2e6f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_zero_shot_no_negative_criteria = \n",
    "\"\"\"\n",
    "You are an AI assistant that validates entries based on specific criteria. \n",
    "\n",
    "Your job is to mark any entries given to you as valid or invalid. An input will be valid whenever it conforms to any of the following sets of criteria.\n",
    "\n",
    "## Valid Business Goals Criteria\n",
    "\n",
    "Meeting all criteria below is a must have for the entry to be considered valid, otherwise it will be invalid\n",
    "\n",
    "    - A) Clarity: the entry mentions a business goal or need that is clear and easy to understand\n",
    "\n",
    "    - B) Specificity: The entry should clearly refer to a specific, concrete business goal or need\n",
    "\n",
    "    - C) Actionable: The entry should focus on the practical applicability of Microsoft 365 products to address the business need described\n",
    "\n",
    "## Response ##\n",
    "\n",
    "You will always respond in JSON format with the following fields:\n",
    "* valid - make it true if the entry is considered valid, false if invalid\n",
    "* reasoning - add your reasoning based on the criteria set above\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed04041e-3f95-48b0-9e11-9c72d339ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = analyse_test_prompt('loose_zero_shot_200_no_negative_criteria', loose_zero_shot_no_negative_criteria, test_results, business_goals_200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "297d60b5-7956-4f99-822e-6afccc7bc8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_name</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loose_zero_shot_20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>loose_zero_shot_20_no_negative_criteria</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.782609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 test_name  sensitivity  precision  f1_score\n",
       "0                       loose_zero_shot_20          1.0   0.615385  0.761905\n",
       "1  loose_zero_shot_20_no_negative_criteria          1.0   0.642857  0.782609"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e79bacc-25d2-4a37-a8f9-80599b828eaf",
   "metadata": {},
   "source": [
    "### 3.2 Detailed Zero Shot Prompt\n",
    "\n",
    "In the prompt below, we provide a detailed list of criteria based on the latest version of the insights framework available at GigPlus. This has a detailed set of tiered criteria. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b399e181-9e25-4bab-8caf-aab7ae61ee57",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "loose_detailed_zero_shot_prompt = '''\n",
    "You are an AI assistant that validates entries based on specific criteria. \n",
    "\n",
    "## PROMPT MISSING ##\n",
    "\n",
    "## Response ##\n",
    "\n",
    "You will respond in JSON format with the following fields:\n",
    "* valid - make it true if the entry is considered valid, false if invalid\n",
    "* reasoning - add your reasoning based on the criteria set above\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797916cf-f235-41a0-b381-9ae277a75bca",
   "metadata": {},
   "source": [
    "### 3.1 Few-Shot Prompt\n",
    "\n",
    "Few-shot prompting is a technique in prompt engineering that aims to augment LLMs by providing a small number of examples within the prompt itself. This allows the model to learn and adapt to a specific task without requiring extensive fine-tuning.\n",
    "\n",
    "In the prompt below, we will provide a few positive and negative examples for each of the categories, and see its impact on performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ec3aae5-8c70-4d8e-a223-7c12f6daf1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = '''\n",
    "\n",
    "You are an AI assistant that validates entries based on specific criteria. \n",
    "\n",
    "Your job is to mark any entries given to you as valid or invalid. An input will be valid whenever it conforms to any of the following sets of criteria.\n",
    "\n",
    "## PROMPT MISSING ##\n",
    "\n",
    "* valid - make it true if the entry is considered valid, false if invalid\n",
    "* reasoning - add your reasoning based on the criteria set above\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f848942d-fe60-4fbb-ac79-38ce92d4a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = analyse_test_prompt('few shot', few_shot_prompt, test_results)\n",
    "\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b6bddd8-8eb8-40d9-b879-1f2798aa6943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_multipass_prompt(test_name, first_prompt, second_prompt, results_store, dataset):\n",
    "    '''\n",
    "    Evaluates the performance of a prompt \n",
    "\n",
    "    Args:\n",
    "      - test_name: name of the test, can be used as an identifier\n",
    "      - first_prompt: system prompt passed to the first validation LLM\n",
    "      - second_prompt: system prompt passed for the second validation using LLM\n",
    "      - results_store: dataframe where we can store the results\n",
    "      - dataset: Cross validation set used for the task\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "\n",
    "    # counters to evaluate metrics\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    llm_call_counter = 0\n",
    "        \n",
    "    for index, row in dataset.iterrows():\n",
    "            # avoid token limit if needed every 10 rows \n",
    "        print(llm_call_counter)\n",
    "        if llm_call_counter >= 10:\n",
    "            print(f\"Rate limit is close, continuing in {60} seconds...\")\n",
    "            time.sleep(61)\n",
    "            llm_call_counter = 0\n",
    "            \n",
    "            \n",
    "        llm_res = send_prompt(first_prompt, row['Feedback'])\n",
    "        llm_res = clean_llm_response(llm_res)\n",
    "        llm_call_counter += 1\n",
    "\n",
    "        print(llm_res)\n",
    "        # if we did not get a TP or an TN, we use the other prompt\n",
    "        llm_res = json.loads(llm_res)\n",
    "        if (llm_res['valid'] == False and human_validation == 1) or (llm_res['valid'] == True and human_validation == 0):\n",
    "            llm_res = send_prompt(prompt_deployment, row['Feedback'])\n",
    "            llm_res = clean_llm_response(llm_res)\n",
    "            try:\n",
    "                llm_res = json.loads(llm_res)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"[WARN] eError parsing JSON: {e}, moving to next case\")  # Log the error\n",
    "                continue\n",
    "                \n",
    "            llm_call_counter += 1\n",
    "            \n",
    "        \n",
    "        \n",
    "        print(llm_res)\n",
    "        \n",
    "        if llm_res['valid'] == True and human_validation == 1:\n",
    "            tp += 1\n",
    "        elif llm_res['valid'] == False and human_validation == 0:\n",
    "            tn += 1\n",
    "        elif llm_res['valid'] == True and human_validation == 0:\n",
    "            fp += 1\n",
    "        elif llm_res['valid'] == False and human_validation == 1:\n",
    "            tn += 1\n",
    "\n",
    "    sensitivity = tp / ( tp + fn )\n",
    "    precision = tp / ( tp + fp )\n",
    "    f_1 = 2 * ( precision * sensitivity ) / ( precision + sensitivity )\n",
    "    new_results_row = pd.DataFrame({\"test_name\": [test_name] ,\"sensitivity\": [sensitivity] , \"precision\": [precision] , \"f1_score\": [f_1]  })\n",
    "\n",
    "    test_values = results_store[\"test_name\"].values\n",
    "    \n",
    "    if test_name in test_values:\n",
    "        index_to_replace = results_store[results_store[\"test_name\"] == test_name].index[0]\n",
    "        results_store.loc[index_to_replace] = new_results_row.iloc[0] \n",
    "    else:\n",
    "        results_store = pd.concat([results_store, new_results_row], ignore_index=True)\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time) \n",
    "    return results_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f22c1-7daa-478b-8ae4-e247dd972308",
   "metadata": {},
   "source": [
    "Now, let's break down the prompts with their examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef4f40-9956-480f-9b5c-ecca029f1586",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Here we have the performance of several prompting strategies on our makeshift cross validation sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "090d22bd-8fa3-4f7a-a731-54e05ee1d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print test results store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6c11bf-8fb1-4701-b59d-15f82997b565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
