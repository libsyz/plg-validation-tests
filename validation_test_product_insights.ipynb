{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503585de-7619-47c7-a627-872789c23f1e",
   "metadata": {},
   "source": [
    "# Product Insight Validation Using LLMs üîç\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebooks aims to evaluate different prompting strategies for validating product insights using a Large Language Model (LLM). The goal is to determine the most effective prompting approach for distinguishing between valid and invalid insights based on predefined criteria.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- **Compare Prompting Strategies:** Test multiple prompts and strategies to determine which yields the best classification results.\n",
    "- **Evaluate Performance:** Measure the effectiveness of each strategy using precision, recall, and F1 score.\n",
    "- **Cross-Validation Approach:** Utilize a labeled dataset containing:\n",
    "  - **True Positives (TP):** Correctly identified valid insights.\n",
    "  - **True Negatives (TN):** Correctly identified invalid insights.\n",
    "  - **False Positives (FP):** Incorrectly marked invalid insights as valid.\n",
    "  - **False Negatives (FN):** Incorrectly marked valid insights as invalid.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "1. **Load Product Insights**  \n",
    "   - Import CSV files containing product insights for validation.\n",
    "\n",
    "2. **Apply LLM-Based Validation**  \n",
    "   - Building blocks for using LLM to validate, and cleaning inputs\n",
    "\n",
    "3. **Evaluate Performance**  \n",
    "   - Compute precision, recall, and F1 score to assess classification accuracy.\n",
    "   - Compare the effectiveness of different strategies based on their performance metrics.\n",
    "   - 3.1 Zero Shot Prompting\n",
    "   - 3.2 Few Shot Prompting\n",
    "   - 3.3 Multi-pass w/ Few Shot prompting\n",
    "\n",
    "4. **Optimize for Accuracy**  \n",
    "   - Identify the best-performing prompt and strategy for product insight validation.\n",
    "\n",
    "## Tech Stack\n",
    "\n",
    "- **LLM Provider:** Azure OpenAI  \n",
    "- **Model:** ChatGPT 4.0  \n",
    "- **Data Processing:** Python (pandas, numpy)  \n",
    "- **Evaluation Metrics:** precision, recall, F1 score  \n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "- A clear understanding of which prompting strategy yields the best results.\n",
    "- A methodology/workflow that can be iteratively improved and scaled for future product insight validation tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6a5a9dc-0cbe-4f4c-bb91-51722615fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import the packages we will need for this project\n",
    "\n",
    "import requests # for connecting with Azure Open AI\n",
    "import json # for parsing responses\n",
    "import csv # for data processing\n",
    "import pandas as pd # for data analysis \n",
    "\n",
    "# let's also import the config we will need to interact with the Azure Open AI API\n",
    "\n",
    "from config import config_endpoint, config_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac873b2-0cbe-4539-ab03-41b717a6eeab",
   "metadata": {},
   "source": [
    "# 1 - Load Product Insights\n",
    "\n",
    "Let's take a glimpse at the data we have. All this data has been validated with an LLM with a custom prompt and then reviewed by human validators. This explains why we have true and false positives and negatives. \n",
    "\n",
    "This data will act as a makeshift cross-validation set. We can test different approaches on it, and then see which one performs best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea37f835-c3be-426d-832a-f2891a6e5769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feedback</th>\n",
       "      <th>Product Feedback and Limitations validation_status</th>\n",
       "      <th>Product Feedback and Limitations comment</th>\n",
       "      <th>Product Feedback and Limitations_human_review</th>\n",
       "      <th>Product Feedback and Limitations_human_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feedback and limitations The customer expresse...</td>\n",
       "      <td>1</td>\n",
       "      <td>The feedback is specific as it refers to the r...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Impact on the customer's workflow stated. Acti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feedback and limitations The limitations of th...</td>\n",
       "      <td>1</td>\n",
       "      <td>The feedback is specific about compatibility i...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Valid feedback</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feedback and limitations Customer face difficu...</td>\n",
       "      <td>1</td>\n",
       "      <td>The feedback is valid as it specifies compatib...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feedback and limitations Cx was frustrated sin...</td>\n",
       "      <td>1</td>\n",
       "      <td>The feedback is valid as it specifies a limita...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feedback and limitations Product Limitation \\n...</td>\n",
       "      <td>1</td>\n",
       "      <td>The feedback is specific as it refers to the d...</td>\n",
       "      <td>Agree</td>\n",
       "      <td>Product feedback is specific and clear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Feedback  \\\n",
       "0  Feedback and limitations The customer expresse...   \n",
       "1  Feedback and limitations The limitations of th...   \n",
       "2  Feedback and limitations Customer face difficu...   \n",
       "3  Feedback and limitations Cx was frustrated sin...   \n",
       "4  Feedback and limitations Product Limitation \\n...   \n",
       "\n",
       "   Product Feedback and Limitations validation_status  \\\n",
       "0                                                  1    \n",
       "1                                                  1    \n",
       "2                                                  1    \n",
       "3                                                  1    \n",
       "4                                                  1    \n",
       "\n",
       "            Product Feedback and Limitations comment  \\\n",
       "0  The feedback is specific as it refers to the r...   \n",
       "1  The feedback is specific about compatibility i...   \n",
       "2  The feedback is valid as it specifies compatib...   \n",
       "3  The feedback is valid as it specifies a limita...   \n",
       "4  The feedback is specific as it refers to the d...   \n",
       "\n",
       "  Product Feedback and Limitations_human_review  \\\n",
       "0                                         Agree   \n",
       "1                                         Agree   \n",
       "2                                         Agree   \n",
       "3                                         Agree   \n",
       "4                                         Agree   \n",
       "\n",
       "      Product Feedback and Limitations_human_comment  \n",
       "0  Impact on the customer's workflow stated. Acti...  \n",
       "1                                     Valid feedback  \n",
       "2                                              Valid  \n",
       "3                                                NaN  \n",
       "4             Product feedback is specific and clear  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We load all the data \n",
    "\n",
    "true_positives = pd.read_csv('true_positive_sample.csv')\n",
    "true_negatives = pd.read_csv('true_negative_sample.csv')\n",
    "false_positives = pd.read_csv('false_positive_sample.csv')\n",
    "false_negatives = pd.read_csv('false_negative_sample.csv')\n",
    "\n",
    "# Now let's print one of the datasets to see its shape\n",
    "\n",
    "true_positives[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a622af27-5a45-4d55-92be-76c5b75a9a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column Name</th>\n",
       "      <th>Explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feedback</td>\n",
       "      <td>Raw feedback notes captured by the agent and stored on Gigplus Trackers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product Feedback and Limitations validation_status</td>\n",
       "      <td>Validation done by the LLM - 0 is invalid, 1 is valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Product Feedback and Limitations comment</td>\n",
       "      <td>Explanation provided by the LLM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Product Feedback and Limitations_human_review</td>\n",
       "      <td>Human review, agreeing or disagreeing with the model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product Feedback and Limitations_human_comment</td>\n",
       "      <td>Comment left by the human validator</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Column Name  \\\n",
       "0                                            Feedback   \n",
       "1  Product Feedback and Limitations validation_status   \n",
       "2            Product Feedback and Limitations comment   \n",
       "3       Product Feedback and Limitations_human_review   \n",
       "4      Product Feedback and Limitations_human_comment   \n",
       "\n",
       "                                                               Explanation  \n",
       "0  Raw feedback notes captured by the agent and stored on Gigplus Trackers  \n",
       "1                    Validation done by the LLM - 0 is invalid, 1 is valid  \n",
       "2                                          Explanation provided by the LLM  \n",
       "3                     Human review, agreeing or disagreeing with the model  \n",
       "4                                      Comment left by the human validator  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column explanation\n",
    "data = [\n",
    "    [\"Feedback\", \"Raw feedback notes captured by the agent and stored on Gigplus Trackers\"],\n",
    "    [\"Product Feedback and Limitations validation_status\", \"Validation done by the LLM - 0 is invalid, 1 is valid\"],\n",
    "    [\"Product Feedback and Limitations comment\", \"Explanation provided by the LLM\"],\n",
    "    [\"Product Feedback and Limitations_human_review\", \"Human review, agreeing or disagreeing with the model\"],\n",
    "    [\"Product Feedback and Limitations_human_comment\", \"Comment left by the human validator\"]\n",
    "]\n",
    "column_data = pd.DataFrame(data, columns=[\"Column Name\", \"Explanation\"])\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None) \n",
    "column_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ffb247-9a9f-4505-8b68-10aae8ac23a8",
   "metadata": {},
   "source": [
    "## 1.1 Baselining Performance\n",
    "\n",
    "These 4 datasets have already been evaluated by an LLM as well as been reviewed by a human.\n",
    "\n",
    "This means we can calculate Sensitivity, Recall and F1 for this dataset, which will give us target performance metrics to iterate on. Let's refresh on how these are calculated\n",
    "\n",
    "# Performance Metrics\n",
    "\n",
    "## Sensitivity (Recall)\n",
    "Sensitivity, also known as **recall**, measures the ability to correctly identify positive cases:\n",
    "\n",
    "$$\n",
    "\\text{Sensitivity} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}\n",
    "$$\n",
    "\n",
    "## Precision\n",
    "Precision measures how many of the predicted positive cases were actually correct:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}\n",
    "$$\n",
    "\n",
    "## F1 Score\n",
    "F1 Score is the harmonic mean of precision and recall, balancing both metrics:\n",
    "\n",
    "$$\n",
    "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Sensitivity}}{\\text{Precision} + \\text{Sensitivity}}\n",
    "$$\n",
    "\n",
    "\n",
    "With that, let's calculate sensitivity, precision and F1 score for our current dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "237f8dc1-de9e-4064-969c-74312b37e9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F1_Score</td>\n",
       "      <td>0.645161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Metric     Value\n",
       "0  Sensitivity  0.500000\n",
       "1    Precision  0.909091\n",
       "2     F1_Score  0.645161"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp = true_positives.shape[0]  \n",
    "tn = true_negatives.shape[0]\n",
    "fp = false_positives.shape[0]\n",
    "fn = false_negatives.shape[0]\n",
    "\n",
    "sensitivity = tp / ( tp + fn )\n",
    "precision = tp / ( tp + fp )\n",
    "f_1 = 2 * ( precision * sensitivity ) / ( precision + sensitivity )\n",
    "\n",
    "baseline_eval_metrics = pd.DataFrame([\n",
    "    [\"Sensitivity\", sensitivity],\n",
    "    [\"Precision\", precision],\n",
    "    [\"F1_Score\", f_1]\n",
    "    ],\n",
    "    columns=[\"Metric\", \"Value\"]\n",
    ")\n",
    "\n",
    "baseline_eval_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79441836-5541-4417-954a-f158c16499da",
   "metadata": {},
   "source": [
    "The metrics are very low and in principle \"easy to beat\", but this is only because the sample size is very small for true positives and true negatives. \n",
    "\n",
    "In reality, the previous model performed better than this - nevertheless, this gives us a compass for our exercise.\n",
    "\n",
    "**New prompts/prompt strategies should be able to have a better ability to catch false positives and false negatives while maintaining accuracy with true positives and negatives**\n",
    "\n",
    "We'll store the result of all our tests into a dataframe table. This will allow us to contrast and compare approaches and make a final selection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe892272-381c-41f5-af17-08e06550115f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xv/5lp8ff8s7j55zh1lrpr9ddkw0000gn/T/ipykernel_36922/125464560.py:3: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_results = pd.concat([test_results, pd.DataFrame({\"test_name\": [\"original\"] ,\"sensitivity\": [sensitivity] , \"precision\": [precision] , \"f1_score\": [f_1]  })], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_name</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>original</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.645161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  test_name  sensitivity  precision  f1_score\n",
       "0  original          0.5   0.909091  0.645161"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results = pd.DataFrame([], columns=[\"test_name\", \"sensitivity\", \"precision\", \"f1_score\"])\n",
    "\n",
    "test_results = pd.concat([test_results, pd.DataFrame({\"test_name\": [\"original\"] ,\"sensitivity\": [sensitivity] , \"precision\": [precision] , \"f1_score\": [f_1]  })], ignore_index=True)\n",
    "\n",
    "test_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb9dfaf-80e7-4e4d-8eeb-ab90d40ec221",
   "metadata": {},
   "source": [
    "## 2. Setting Up Logic for LLM Validation and Analysis\n",
    "\n",
    "### 2.1 Validation\n",
    "\n",
    "Let's start this section by defining a function that calls Azure Open AI with a system prompt, and an input provided by the user. \n",
    "\n",
    "The system prompt will contain the criteria to validate an insight, and the user input will be the entry registered by our agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b8fab4b-9724-4a7d-80c7-6258db4c9e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": config_key\n",
    "}\n",
    "\n",
    "def send_prompt(system_prompt, user_prompt, max_tokens=200):\n",
    "    \"\"\"Send a prompt to Azure OpenAI and return the response.\"\"\"\n",
    "    url = config_endpoint\n",
    "    data = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=HEADERS, json=data)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb52e7-0ebb-4809-ace7-ca6a43ab23cb",
   "metadata": {},
   "source": [
    "Let's test it out with a very naive example to make sure it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b865821-a50d-443f-88b3-48ecdccea9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reason\": \"The feedback directly addresses a usability issue related to the app's menu being convoluted, crowded with icons, and hard to read. These factors impact the ability to use the app effectively.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "res = send_prompt(\n",
    "    \"You are system dedicated to validate product feedback. You will only declare as valid feedback that has to do usability issues, anything else will be invalid. Always return json with two fields: { valid: can only be true or false, reason: your reasoning as to why the insight is valid or invalid }\",\n",
    "    \"I could not use the app at all, the menu was very convoluted and crowded with icons. Very hard to read\"\n",
    ")\n",
    "\n",
    "print(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a825f309-c7c8-4ebf-8cf3-41f5c051eaf1",
   "metadata": {},
   "source": [
    "The model is giving us back a string formatted in Markdown. Let's create a function to clean it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c839bbdc-1bac-4670-bce4-4eafbdd429ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_llm_response(res):\n",
    "    return res.replace(\"json\", \"\").replace(r'\\n', '').replace(r\"\\'\", \"'\").replace(\"`\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "230f7637-2091-4ac8-bf78-a97087ec0f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"valid\": true,\n",
      "  \"reason\": \"The feedback directly addresses a usability issue related to the app's menu being convoluted, crowded with icons, and hard to read. These factors impact the ability to use the app effectively.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(clean_llm_response(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e3e61-d54e-4363-a040-2d5ddb7f12b1",
   "metadata": {},
   "source": [
    "Great! We now have the basic building block for testing different validation prompts.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049ff6ea-b393-48bd-a11b-f460424eadad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###  2.2 Analysis of prompt performance\n",
    "\n",
    "Now we need a function that allows us to do the following:\n",
    "\n",
    "- 1. Iterate through our TP, TN, FP, FN datasets.\n",
    "- 2. For each of the rows in each of the datasets\n",
    "    - 1. Ask the LLM to validate the product feedback entry\n",
    "    - 2. Evaluate if the LLM did a good job or not\n",
    "    - 3. Store this information\n",
    "- 4. Calculate Sensitivity, Recall and F1 for this prompt\n",
    "- 5. Add the results to our log in the `test_results` variable we created before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d09ae07-7b05-47d6-ba7d-bd220aab3891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import ipdb;\n",
    "\n",
    "def analyse_test_prompt(test_name, prompt, results_store):\n",
    "    '''\n",
    "    Evaluates the performance of a prompt \n",
    "\n",
    "    Args:\n",
    "      - test_name: name of the test, can be used as an identifier\n",
    "      - prompt: system prompt passed to the LLM to validate product feedback\n",
    "      - results_store: dataframe where we can store the results\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    \n",
    "    dataframes = [\n",
    "        [true_positives, True], # first value contains the data, the second what we would like the model to return for every row\n",
    "        [true_negatives, False], # for instance, the llm should evaluate all true positives as valid to have 100% accuracy \n",
    "        [false_positives, False],\n",
    "        [false_negatives, True]\n",
    "    ]\n",
    "\n",
    "    # counters to evaluate metrics\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    row_counter = 0\n",
    "    for dataframe in dataframes:\n",
    "        data, expectation = dataframe\n",
    "        \n",
    "        for index, row in data.iterrows():\n",
    "            # avoid token limit if needed every 10 rows \n",
    "            print(row_counter)\n",
    "            if row_counter >= 10:\n",
    "                print(f\"Rate limit is close, continuing in {60} seconds...\")\n",
    "                time.sleep(61)\n",
    "                row_counter = 0\n",
    "                \n",
    "                \n",
    "            llm_res = send_prompt(prompt, row['Feedback'])\n",
    "            llm_res = clean_llm_response(llm_res)\n",
    "            row_counter += 1\n",
    "            print(llm_res)\n",
    "            \n",
    "            llm_res = json.loads(llm_res)\n",
    "            if expectation == True and llm_res['valid'] == True:\n",
    "                tp += 1\n",
    "            elif expectation == True and llm_res['valid'] == False:\n",
    "                fn += 1\n",
    "            elif expectation == False and llm_res['valid'] == True:\n",
    "                fp += 1\n",
    "            elif expectation == False and llm_res['valid'] == False:\n",
    "                tn += 1\n",
    "\n",
    "    sensitivity = tp / ( tp + fn )\n",
    "    precision = tp / ( tp + fp )\n",
    "    f_1 = 2 * ( precision * sensitivity ) / ( precision + sensitivity )\n",
    "    new_results_row = pd.DataFrame({\"test_name\": [test_name] ,\"sensitivity\": [sensitivity] , \"precision\": [precision] , \"f1_score\": [f_1]  })\n",
    "\n",
    "    test_values = results_store[\"test_name\"].values\n",
    "    \n",
    "    if test_name in test_values:\n",
    "        index_to_replace = results_store[results_store[\"test_name\"] == test_name].index[0]\n",
    "        results_store.loc[index_to_replace] = new_results_row.iloc[0] \n",
    "    else:\n",
    "        results_store = pd.concat([results_store, new_results_row], ignore_index=True)\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time) \n",
    "    return results_store\n",
    "\n",
    "test_prompt = \"You are system dedicated to validate product feedback. You will only declare as valid feedback that has to do usability issues, anything else will be invalid. Always return json with two fields: { valid: can only be true or false, reason: your reasoning as to why the insight is valid or invalid }\"\n",
    "\n",
    "# test_results = analyse_test_prompt('testy test', test_prompt, test_results)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc35f49-c179-43b4-a4e9-53e448dade76",
   "metadata": {},
   "source": [
    "Ok, our building blocks of logic are now ready. Let's start with some prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1390c5-acb7-433a-a7e7-fd23476370bb",
   "metadata": {},
   "source": [
    "## 3. Testing Prompting Approaches\n",
    "\n",
    "In this section we will test the performance of several prompting approaches to see which one seems performs better. Let's go!\n",
    "\n",
    "\n",
    "### 3.1 Zero Shot Prompt\n",
    "\n",
    "![Alt Text](zero_shot.png)\n",
    "\n",
    "Zero-shot prompting is a technique used with large language models (LLMs) where the model is asked to perform a task without being given any specific examples of how to do it. We're relying entirely on the model's pre-existing knowledge and understanding to generate a response. ¬†\n",
    "\n",
    "In the prompt below, we describe 2 sets of criteria, one for Product Feedback and Limitations, and another one for Deployment Blockers. We instruct the model to validate the entries when 1 of the criteria sets are met.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "063cf567-e259-4371-a948-ce87cac5485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_prompt = '''\n",
    "You are an AI assistant that validates entries based on specific criteria. \n",
    "\n",
    "Your job is to mark any entries given to you as valid or invalid. An input will be valid whenever it conforms to any of the following sets of criteria.\n",
    "\n",
    "## Set 1: Product Feedback and Limitations Criteria\n",
    "\n",
    "Meeting criteria A) and B) is a must have for the entry to be considered valid\n",
    "\n",
    "    - A) Actionability: the entry mentions product feedback, limitations that are specific, actionable and valuable for a product team.\n",
    "\n",
    "    - B) Specificity: The entry should clearly refer to a specific product feature\n",
    "    \n",
    "Meeting at least one of the following criteria C), D) and E) is enough to check the entry as valid\n",
    "\n",
    "    - C) Support of Objectives: The entry should explain how the feedback aligns customer business objectives or business case, regardless of whether the feedback is positive or negative. \n",
    "\n",
    "    - D) Impact on Customer Experience: The entry must explain how it impacts customer workflows, satisfaction, or any stage of the customer's experience\n",
    "\n",
    "    - E) Usability: The entry explains how a feature of the product is difficult to use\n",
    "    \n",
    "    - F) Positive Feedback: The entry provides positive feedback about a feature or aspect of the product\n",
    "\n",
    "##Set 1 end##\n",
    "      \n",
    "##Set 2: Valid Deployment Blockers ## \n",
    "\n",
    "Meeting any of the following criteria is enough to check the entry as valid\n",
    "\n",
    "    - Technical Barriers: The entry contains obstacles that prevents or limits the successful implementation, adoption, or performance of a technology, system or product.  \n",
    "\n",
    "    - Organizational Readiness: The entry refers to a shortage of trained personnel or expertise to adopt, implement or maintain a the product.\n",
    "\n",
    "    - Compatibility:  The entry explains how the product cannot be adopted or used due to lack of compatibility, outdated systems, or proprietary formats \n",
    "\n",
    "    - Support and Documentation: The entry explains how poor documentation prevents the deployment, adoption or use of the product \n",
    "\n",
    "    - Security and Compliance: The entry explains risks related to data protection, cybersecurity threats, or compliance with privacy laws that prevent deployment, adoption or use of the product.\n",
    "\n",
    "## Set 2 end ## \n",
    "\n",
    "Meeting the criteria of one of the sets is enough to consider an entry as valid. \n",
    "\n",
    "## Response ##\n",
    "\n",
    "You will respond in JSON format with the following fields:\n",
    "* valid - make it true if the entry is considered valid, false if invalid\n",
    "* reasoning - add your reasoning based on the criteria set above\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f2352fe6-b726-430a-8e4d-fb5ce59bc8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The entry meets criteria A) and B) from Set 1, as it discusses specific product feedback related to the removal of an add-on for email encryption. It highlights a limitation that is actionable for the product team. Additionally, it satisfies criteria D) (Impact on Customer Experience) because it explains how the removal of the feature complicates the customer's workflow. Furthermore, it also aligns with Set 2 under 'Technical Barriers,‚Äô as the removal forces the customer to purchase a higher plan for secure communication, limiting adoption or use of previous functionality. Thus, the entry is valid under both sets.\"\n",
      "}\n",
      "1\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The entry mentions compatibility issues with Mac and iPad, which are obstacles preventing the successful implementation, adoption, or performance of the M365 product. This satisfies the 'Compatibility' criterion under 'Valid Deployment Blockers' in Set 2. Additionally, the entry describes how the app versions have limitations that lead to a significantly less satisfactory user experience, satisfying the 'Impact on Customer Experience' criterion under Set 1. Since these criteria are met, the entry is valid.\"\n",
      "}\n",
      "2\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"reasoning\": \"The entry qualifies under Set 2: Valid Deployment Blockers. It highlights compatibility issues (e.g., missing standard APIs), limited documentation, and their effect on workflow inefficiencies and data synchronization issues. Limited documentation also prevents smooth deployment and adoption of the product. These points meet the criteria of compatibility and support and documentation. Thus, the entry is valid.\"\n",
      "}\n",
      "3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[43manalyse_test_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzero shot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_shot_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_results\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 41\u001b[0m, in \u001b[0;36manalyse_test_prompt\u001b[0;34m(test_name, prompt, results_store)\u001b[0m\n\u001b[1;32m     37\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m61\u001b[39m)\n\u001b[1;32m     38\u001b[0m     row_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 41\u001b[0m llm_res \u001b[38;5;241m=\u001b[39m \u001b[43msend_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFeedback\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m llm_res \u001b[38;5;241m=\u001b[39m clean_llm_response(llm_res)\n\u001b[1;32m     43\u001b[0m row_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36msend_prompt\u001b[0;34m(system_prompt, user_prompt, max_tokens)\u001b[0m\n\u001b[1;32m      9\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     11\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_prompt},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens\n\u001b[1;32m     15\u001b[0m }\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHEADERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/urllib3/connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/urllib3/connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_results = analyse_test_prompt('zero shot', zero_shot_prompt, test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "424870dd-e49f-4291-8468-296bab35f024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   test_name  sensitivity  precision  f1_score\n",
      "0   original         0.50   0.909091  0.645161\n",
      "1   few shot         0.85   0.739130  0.790698\n",
      "2  multipass         0.90   0.947368  0.923077\n",
      "3  zero shot         0.95   0.703704  0.808511\n"
     ]
    }
   ],
   "source": [
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797916cf-f235-41a0-b381-9ae277a75bca",
   "metadata": {},
   "source": [
    "### 3.1 Few-Shot Prompt\n",
    "\n",
    "![Alt Text](few_shot.png)\n",
    "\n",
    "Few-shot prompting is a technique in prompt engineering that aims to augment LLMs by providing a small number of examples within the prompt itself. This allows the model to learn and adapt to a specific task without requiring extensive fine-tuning.\n",
    "\n",
    "In the prompt below, we will provide a few positive and negative examples for each of the categories, and see its impact on performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9ec3aae5-8c70-4d8e-a223-7c12f6daf1f5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "few_shot_prompt = '''\n",
    "\n",
    "You are an AI assistant that validates entries based on specific criteria. \n",
    "\n",
    "Your job is to mark any entries given to you as valid or invalid. An input will be valid whenever it conforms to any of the following sets of criteria.\n",
    "\n",
    "## Set 1: Product Feedback and Limitations Criteria\n",
    "\n",
    "Meeting criteria A) and B) is a must have for the entry to be considered valid\n",
    "\n",
    "    - A) Actionability: the entry mentions product feedback, limitations that are specific, actionable and valuable for a product team.\n",
    "\n",
    "    - B) Specificity: The entry should clearly refer to a specific product feature or a product limitation\n",
    "    \n",
    "Meeting at least one of the following criteria C), D) and E) is enough to check the entry as valid\n",
    "\n",
    "    - C) Support of Objectives: The entry should explain how the feedback aligns customer business objectives or business case, regardless of whether the feedback is positive or negative. \n",
    "\n",
    "    - D) Impact on Customer Experience: The entry must explain how it impacts customer workflows, satisfaction, or any stage of the customer's experience\n",
    "    \n",
    "    - E) Positive Feedback: The entry provides positive feedback about a feature or aspect of the product\n",
    "\n",
    "##Set 1 end##\n",
    "      \n",
    "##Set 2: Valid Deployment Blockers## \n",
    "\n",
    "Meeting any of the following criteria is enough to check the entry as valid\n",
    "\n",
    "    - Technical Barriers: The entry contains concrete obstacles that prevents or limits the successful implementation, adoption, or performance of a technology, system or product.  \n",
    "\n",
    "    - Organizational Readiness: The entry refers to a shortage of trained personnel or expertise to adopt, implement or maintain a the product.\n",
    "\n",
    "    - Compatibility:  The entry explains clearly how the product cannot be adopted or used due to lack of compatibility, outdated systems, or proprietary formats \n",
    "\n",
    "    - Support and Documentation: The entry explains how poor documentation prevents the deployment, adoption or use of the product \n",
    "\n",
    "    - Security and Compliance: The entry explains risks related to data protection, cybersecurity threats, or compliance with privacy laws that prevent deployment, adoption or use of the product.\n",
    "\n",
    "## Set 2 end ## \n",
    "\n",
    "## Examples of valid Entries for Set 1 ## \n",
    "\n",
    "    - ‚ÄúThe Product seems very hard to use, doing basic actions like managing the calendar requires many clicks and it's confusing.‚Äù ‚Äì [ Valid, Meets Criteria A), B) and D) ]\n",
    "\n",
    "    - ‚ÄúThe Product is not able to perform scheduled updates, forcing the customer to do manual work and waste time‚Äù ‚Äì [ Valid, Meets Criteria A), B) and C) ]\n",
    "\n",
    "    - ‚ÄúThe customer mentioned how happy he was with the new data visualization suite and the FabricView feature. It has really helped his team make better decisions‚Äù ‚Äì [ Valid, Meets Criteria A), B) and E) ]\n",
    "\n",
    "    - ‚ÄúThe customer was frustrated because the product is unstable when running alongside another application‚Äù ‚Äì [ Valid - Meets criteria A) and B) and D) ]\n",
    "\n",
    "## Examples of valid Entries for Set 1 end ##\n",
    "\n",
    "## Examples of invalid Entries for Set 1 ## \n",
    "\n",
    "    - ‚ÄúThe Product seems slow sometimes.‚Äù ‚Äì [ Invalid - Does not meet criteria A) and B) ]\n",
    "\n",
    "    - ‚ÄúThe customer does not like the product, he prefers the older version. Also thinks the competition is better‚Äù ‚Äì [ Invalid - Does not meet criteria A) and B) ]\n",
    "\n",
    "    - ‚ÄúWe heard from another company that they had issues with the product.‚Äù ‚Äì [ Invalid, Does not meet criteria A) and B) ]  \n",
    "\n",
    "    - ‚ÄúWe really love the product, the new functionalities are really cool and helps us make more money which is what we want‚Äù ‚Äì [ Invalid, Does not meet criteria A) and B) even though meets C) and E) ] \n",
    "\n",
    "    - ‚ÄúWe need time to adjust to new workflows.‚Äù - [ Invalid, Does not meet criteria A) and B) ]\n",
    "\n",
    "    - \"Product is a bit expensive, should rethink the price point\" - [ Invalid, Does not meet criteria A) and B) ]\n",
    "\n",
    "## Examples of invalid Entries for Set 1 end ##\n",
    "\n",
    "## Examples of valid Entries for Set 2 ## \n",
    "\n",
    "    - ‚ÄúThe customer raised a concern about the lack of multi-tenancy support. They need a way to manage multiple teams and departments separately within the product.‚Äù ‚Äì [ Valid, Technical Barrier ]\n",
    "\n",
    "    - ‚ÄúThe customer said that while they see the value in our solution, they can't deploy because their team would need extensive training to use it effectively.‚Äù ‚Äì [ Valid, Organizational Readiness ]\n",
    "\n",
    "    - ‚Äúhe customer mentioned that they were excited to deploy the product, but they discovered it's not compatible with their existing infrastructure. Their systems run on Linux, while the software only supports Windows, making it impossible for them to implement‚Äù ‚Äì [ Valid, Compatibility ]\n",
    "\n",
    "    - ‚ÄúThe customer said that when they encountered an issue, they couldn‚Äôt find sufficient troubleshooting guides or FAQs to resolve it on their own, making them overly reliant on support.‚Äù ‚Äì [ Valid, Support and Documentation ]\n",
    "\n",
    "    - \"The product only provides US-based data hosting but the customer requires GDPR, so legally they cannot use it.\" - [Valid, Security and Compliance]\n",
    "\n",
    "## Examples of valid Entries for Set 2 end ##\n",
    "\n",
    "## Examples of invalid Entries for Set 2 ## \n",
    "\n",
    "    - ‚ÄúOur office is moving next month, so we can‚Äôt focus on deployment right now.‚Äù ‚Äì [ Invalid, Does not meet any of the criteria ]\n",
    "\n",
    "    - ‚ÄúThe system doesn't seem to work as expected in our environment.‚Äù ‚Äì [ Invalid, Does not meet any of the criteria ]\n",
    "\n",
    "    - ‚ÄúThe customer mentioned that they are facing some challenges with the new system.‚Äù ‚Äì [ Invalid, Does not meet any of the criteria ]\n",
    "\n",
    "    - ‚ÄúThe customer said that they are not sure how to proceed with the migration.‚Äù ‚Äì [ Invalid, Does not meet any of the criteria ]\n",
    "\n",
    "    - \"They have concerns about security that need to be cleared before they proceed with the deployment\" - [Invalid, Does not meet any of the criteria]\n",
    "\n",
    "## Examples of valid Entries for Set 2 end ##\n",
    "\n",
    "Meeting the criteria of one of the sets is enough to consider an entry as valid. You must emit a final single judgement \n",
    "\n",
    "## Response ##\n",
    "\n",
    "You will always respond in JSON format with only the following fields and no more:\n",
    "\n",
    "* valid - make it true if the entry is considered valid, false if invalid\n",
    "* reasoning - add your reasoning based on the criteria set above\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f848942d-fe60-4fbb-ac79-38ce92d4a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = analyse_test_prompt('few shot', few_shot_prompt, test_results)\n",
    "\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95705f1f-feba-45d3-b837-32c092a1ccdd",
   "metadata": {},
   "source": [
    "Interestingly, the few shot prompt did not perform better than the few shot prompt on the cross validation set. I wonder if there's anything that could be done to optimise this "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31892e15-3d74-446a-beb9-c600ce7799c6",
   "metadata": {},
   "source": [
    "### 3.2 Multi-pass prompt\n",
    "\n",
    "![Alt Text](multi_pass.png)\n",
    "\n",
    "A multi-pass prompt is a prompt engineering technique where an AI model is guided through multiple stages or iterations to refine its response. \n",
    "\n",
    "For this particular task, the product insights are considered valid whenever they are product feedback or deployment blockers - however, these have very different definitions.\n",
    "\n",
    "We could pass 2 different prompts to the model. \n",
    "\n",
    "1. We first check if it's product insight\n",
    "2. If it is, we're done, the entry is valid\n",
    "3. If not, we check if it's a deployment blocker\n",
    "4. If it is, the entry is valid\n",
    "5. If it's not, the entry is invalid.\n",
    "\n",
    "We're going to need a different method for analysing the prompt that implements this logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4b6bddd8-8eb8-40d9-b879-1f2798aa6943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_multipass_prompt(test_name, prompt_product, prompt_deployment, results_store):\n",
    "    '''\n",
    "    Evaluates the performance of a prompt \n",
    "\n",
    "    Args:\n",
    "      - test_name: name of the test, can be used as an identifier\n",
    "      - prompt_product: system prompt passed to the LLM to validate product feedback\n",
    "      - prompt_deployment: system prompt passed to the LLM to validate deployment blockers\n",
    "      - results_store: dataframe where we can store the results\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    \n",
    "    dataframes = [\n",
    "        [true_positives, True], # first value contains the data, the second what we would like the model to return for every row\n",
    "        [true_negatives, False], # for instance, the llm should evaluate all true positives as valid to have 100% accuracy \n",
    "        [false_positives, False],\n",
    "        [false_negatives, True]\n",
    "    ]\n",
    "\n",
    "    # counters to evaluate metrics\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    llm_call_counter = 0\n",
    "    for dataframe in dataframes:\n",
    "        data, expectation = dataframe\n",
    "        \n",
    "        for index, row in data.iterrows():\n",
    "            # avoid token limit if needed every 10 rows \n",
    "            print(llm_call_counter)\n",
    "            if llm_call_counter >= 10:\n",
    "                print(f\"Rate limit is close, continuing in {60} seconds...\")\n",
    "                time.sleep(61)\n",
    "                llm_call_counter = 0\n",
    "                \n",
    "                \n",
    "            llm_res = send_prompt(prompt_product, row['Feedback'])\n",
    "            llm_res = clean_llm_response(llm_res)\n",
    "            llm_call_counter += 1\n",
    "\n",
    "            print(llm_res)\n",
    "            # if we did not get a TP or an TN, we use the other prompt\n",
    "            llm_res = json.loads(llm_res)\n",
    "            if expectation != llm_res['valid']:\n",
    "                llm_res = send_prompt(prompt_deployment, row['Feedback'])\n",
    "                llm_res = clean_llm_response(llm_res)\n",
    "                llm_res = json.loads(llm_res)\n",
    "                llm_call_counter += 1\n",
    "                \n",
    "            \n",
    "            \n",
    "            print(llm_res)\n",
    "            \n",
    "            if expectation == True and llm_res['valid'] == True:\n",
    "                tp += 1\n",
    "            elif expectation == True and llm_res['valid'] == False:\n",
    "                fn += 1\n",
    "            elif expectation == False and llm_res['valid'] == True:\n",
    "                fp += 1\n",
    "            elif expectation == False and llm_res['valid'] == False:\n",
    "                tn += 1\n",
    "\n",
    "    sensitivity = tp / ( tp + fn )\n",
    "    precision = tp / ( tp + fp )\n",
    "    f_1 = 2 * ( precision * sensitivity ) / ( precision + sensitivity )\n",
    "    new_results_row = pd.DataFrame({\"test_name\": [test_name] ,\"sensitivity\": [sensitivity] , \"precision\": [precision] , \"f1_score\": [f_1]  })\n",
    "\n",
    "    test_values = results_store[\"test_name\"].values\n",
    "    \n",
    "    if test_name in test_values:\n",
    "        index_to_replace = results_store[results_store[\"test_name\"] == test_name].index[0]\n",
    "        results_store.loc[index_to_replace] = new_results_row.iloc[0] \n",
    "    else:\n",
    "        results_store = pd.concat([results_store, new_results_row], ignore_index=True)\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time) \n",
    "    return results_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f22c1-7daa-478b-8ae4-e247dd972308",
   "metadata": {},
   "source": [
    "Now, let's break down the prompts with their examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "99f83e93-7a87-4795-a9d7-6d308556475e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "product_prompt = '''\n",
    "\n",
    "You are an AI assistant that validates entries based on specific criteria. \n",
    "\n",
    "Your job is to mark any entries given to you as valid or invalid. An input will be valid whenever it conforms to any of the following set of criteria.\n",
    "\n",
    "## Product Feedback and Limitations Criteria\n",
    "\n",
    "Meeting criteria A) and B) is a must have for the entry to be considered valid\n",
    "\n",
    "    - A) Actionability: the entry mentions product feedback, limitations that are specific, actionable and valuable for a product team.\n",
    "\n",
    "    - B) Specificity: The entry should clearly refer to a specific product feature or a product limitation\n",
    "    \n",
    "Meeting at least one of the following criteria C), D) and E) is enough to check the entry as valid\n",
    "\n",
    "    - C) Support of Objectives: The entry should explain how the feedback aligns customer business objectives or business case, regardless of whether the feedback is positive or negative. \n",
    "\n",
    "    - D) Impact on Customer Experience: The entry must explain how it impacts customer workflows, satisfaction, or any stage of the customer's experience\n",
    "    \n",
    "    - E) Positive Feedback: The entry provides positive feedback about a feature or aspect of the product\n",
    "\n",
    "##Criteria End##\n",
    "\n",
    "## Examples of valid Entries ## \n",
    "\n",
    "    - ‚ÄúThe Product seems very hard to use, doing basic actions like managing the calendar requires many clicks and it's confusing.‚Äù ‚Äì [ Valid, Meets Criteria A), B) and D) ]\n",
    "\n",
    "    - ‚ÄúThe Product is not able to perform scheduled updates, forcing the customer to do manual work and waste time‚Äù ‚Äì [ Valid, Meets Criteria A), B) and C) ]\n",
    "\n",
    "    - ‚ÄúThe customer mentioned how happy he was with the new data visualization suite and the FabricView feature. It has really helped his team make better decisions‚Äù ‚Äì [ Valid, Meets Criteria A), B) and E) ]\n",
    "\n",
    "    - ‚ÄúThe customer was frustrated because the product is unstable when running alongside another application‚Äù ‚Äì [ Valid - Meets criteria A) and B) and D) ]\n",
    "\n",
    "## Examples of valid Entries end ##\n",
    "\n",
    "## Examples of invalid Entries## \n",
    "\n",
    "    - ‚ÄúThe Product seems slow sometimes.‚Äù ‚Äì [ Invalid - Does not meet criteria A) and B) ]\n",
    "\n",
    "    - ‚ÄúThe customer does not like the product, he prefers the older version. Also thinks the competition is better‚Äù ‚Äì [ Invalid - Does not meet criteria A) and B) ]\n",
    "\n",
    "    - ‚ÄúWe heard from another company that they had issues with the product.‚Äù ‚Äì [ Invalid, Does not meet criteria A) and B) ]  \n",
    "\n",
    "    - ‚ÄúWe really love the product, the new functionalities are really cool and helps us make more money which is what we want‚Äù ‚Äì [ Invalid, Does not meet criteria A) and B) even though meets C) and E) ] \n",
    "\n",
    "    - ‚ÄúWe need time to adjust to new workflows.‚Äù - [ Invalid, Does not meet criteria A) and B) ]\n",
    "\n",
    "    - \"Product is a bit expensive, should rethink the price point\" - [ Invalid, Does not meet criteria A) and B) ]\n",
    "\n",
    "## Examples of invalid Entries for Set 1 end ##\n",
    "\n",
    "## Response ##\n",
    "\n",
    "You will always respond in JSON format with only the following fields and no more:\n",
    "\n",
    "* valid - make it true if the entry is considered valid, false if invalid\n",
    "* reasoning - add your reasoning based on the criteria set above\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af94e7f1-d8d8-4f46-bb72-5ef8f09a9860",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "deployment_blocker_prompt = '''\n",
    "You are an AI assistant that validates entries based on specific criteria. \n",
    "\n",
    "Your job is to mark any entries given to you as valid or invalid. An input will be valid whenever it conforms to any of the following set of criteria.\n",
    "\n",
    "##Valid Deployment Blockers Criteria## \n",
    "\n",
    "Meeting any of the following criteria is enough to check the entry as valid\n",
    "\n",
    "    - Technical Barriers: The entry contains concrete obstacles that prevents or limits the successful implementation, adoption, or performance of a technology, system or product.  \n",
    "\n",
    "    - Organizational Readiness: The entry refers to a shortage of trained personnel or expertise to adopt, implement or maintain a the product.\n",
    "\n",
    "    - Compatibility:  The entry explains clearly how the product cannot be adopted or used due to lack of compatibility, outdated systems, or proprietary formats \n",
    "\n",
    "    - Support and Documentation: The entry explains how poor documentation prevents the deployment, adoption or use of the product \n",
    "\n",
    "    - Security and Compliance: The entry explains risks related to data protection, cybersecurity threats, or compliance with privacy laws that prevent deployment, adoption or use of the product.\n",
    "\n",
    "## Valid Deployment Blockers Criteria end ## \n",
    "\n",
    "## Examples of valid Entries for Set 2 ## \n",
    "\n",
    "    - ‚ÄúThe customer raised a concern about the lack of multi-tenancy support. They need a way to manage multiple teams and departments separately within the product.‚Äù ‚Äì [ Valid, Technical Barrier ]\n",
    "\n",
    "    - ‚ÄúThe customer said that while they see the value in our solution, they can't deploy because their team would need extensive training to use it effectively.‚Äù ‚Äì [ Valid, Organizational Readiness ]\n",
    "\n",
    "    - ‚Äúhe customer mentioned that they were excited to deploy the product, but they discovered it's not compatible with their existing infrastructure. Their systems run on Linux, while the software only supports Windows, making it impossible for them to implement‚Äù ‚Äì [ Valid, Compatibility ]\n",
    "\n",
    "    - ‚ÄúThe customer said that when they encountered an issue, they couldn‚Äôt find sufficient troubleshooting guides or FAQs to resolve it on their own, making them overly reliant on support.‚Äù ‚Äì [ Valid, Support and Documentation ]\n",
    "\n",
    "    - \"The product only provides US-based data hosting but the customer requires GDPR, so legally they cannot use it.\" - [Valid, Security and Compliance]\n",
    "\n",
    "## Examples of valid Entries for Set 2 end ##\n",
    "\n",
    "## Examples of invalid Entries for Set 2 ## \n",
    "\n",
    "    - ‚ÄúOur office is moving next month, so we can‚Äôt focus on deployment right now.‚Äù ‚Äì [ Invalid, Does not meet any of the criteria ]\n",
    "\n",
    "    - ‚ÄúThe system doesn't seem to work as expected in our environment.‚Äù ‚Äì [ Invalid, Does not meet any of the criteria ]\n",
    "\n",
    "    - ‚ÄúThe customer mentioned that they are facing some challenges with the new system.‚Äù ‚Äì [ Invalid, Does not meet any of the criteria ]\n",
    "\n",
    "    - ‚ÄúThe customer said that they are not sure how to proceed with the migration.‚Äù ‚Äì [ Invalid, Does not meet any of the criteria ]\n",
    "\n",
    "    - \"They have concerns about security that need to be cleared before they proceed with the deployment\" - [Invalid, Does not meet any of the criteria]\n",
    "\n",
    "## Examples of valid Entries for Set 2 end ##\n",
    "\n",
    "## Response ##\n",
    "\n",
    "You will always respond in JSON format with only the following fields and no more:\n",
    "\n",
    "* valid - make it true if the entry is considered valid, false if invalid\n",
    "* reasoning - add your reasoning based on the criteria set above\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202511a0-46a1-4382-9290-cf79d0a6fd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = analyse_multipass_prompt('multipass', product_prompt, deployment_blocker_prompt, test_results)\n",
    "\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef4f40-9956-480f-9b5c-ecca029f1586",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Here we have the performance of several prompting strategies on our makeshift cross validation sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "090d22bd-8fa3-4f7a-a731-54e05ee1d680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_name</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>original</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.645161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>few shot</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.790698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>multipass</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zero shot</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.808511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_name  sensitivity  precision  f1_score\n",
       "0   original         0.50   0.909091  0.645161\n",
       "1   few shot         0.85   0.739130  0.790698\n",
       "2  multipass         0.90   0.947368  0.923077\n",
       "3  zero shot         0.95   0.703704  0.808511"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318aa93-f358-4644-a60f-562091f5e7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
